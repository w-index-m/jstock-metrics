import streamlit as st
import google.generativeai as genai
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from dateutil.relativedelta import relativedelta
from groq import Groq
import requests
import xml.etree.ElementTree as ET
import re
from io import StringIO
import streamlit.components.v1 as components
import html
# ===========================
# Google Analytics
# ===========================
GA_MEASUREMENT_ID = st.secrets.get("GA_MEASUREMENT_ID", "")

def sanitize_html(text: str) -> str:
    return html.escape(text, quote=True)
def inject_ga():
    """Google Analyticsã‚¿ã‚°ã‚’æ³¨å…¥"""
    if not GA_MEASUREMENT_ID or not GA_MEASUREMENT_ID.startswith("G-"):
        return

    components.html(
        f"""
        <script async src="https://www.googletagmanager.com/gtag/js?id={sanitize_html(GA_MEASUREMENT_ID)}"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){{dataLayer.push(arguments);}}
          gtag('js', new Date());
          gtag('config', '{sanitize_html(GA_MEASUREMENT_ID)}', {{
              'send_page_view': false
          }});
        </script>
        """,
        height=0,
        width=0,
    )
inject_ga()
def track_page_view():
    if not GA_MEASUREMENT_ID:
        return

    components.html(
        """
        <script>
        if (typeof gtag !== 'undefined') {
            gtag('event', 'page_view', {
                page_title: document.title,
                page_location: window.location.href
            });
        }
        </script>
        """,
        height=0,
        width=0,
    )

track_page_view()
# -----------------------------
# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
# -----------------------------
import matplotlib.font_manager as fm
import os

_FONT_PATHS = [
    "font/NotoSansCJK-Regular.ttc",
    "font/NotoSansJP-ExtraBold.ttf",
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc",
    "/usr/share/fonts/truetype/fonts-japanese-gothic.ttf",
    "/usr/share/fonts/opentype/ipafont-gothic/ipag.ttf",
]

def _set_japanese_font():
    for path in _FONT_PATHS:
        if os.path.exists(path):
            fm.fontManager.addfont(path)
            prop = fm.FontProperties(fname=path)
            plt.rcParams["font.family"] = prop.get_name()
            return prop.get_name()
    return None

_set_japanese_font()
plt.rcParams["axes.unicode_minus"] = False

# -----------------------------
# å®šæ•°
# -----------------------------
GEMINI_MODEL = "gemini-2.5-pro"
GROQ_MODEL   = "llama-3.3-70b-versatile"

# -----------------------------
# ãƒšãƒ¼ã‚¸è¨­å®š
# -----------------------------
st.set_page_config(layout="wide", page_title="ğŸ“ˆ æ—¥æœ¬æ ª åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ æ—¥æœ¬æ ª ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªåˆ†æ + ãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆ")

# -----------------------------
# AIè¨­å®šï¼ˆGeminiå„ªå…ˆ / Groqãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
# -----------------------------
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
GROQ_API_KEY   = st.secrets.get("GROQ_API_KEY", "")
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel(GEMINI_MODEL)
groq_client  = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

OPENROUTER_API_KEY = st.secrets.get("OPENROUTER_API_KEY", "")

def generate_ai_comment(prompt: str) -> tuple[str, str]:
    """Gemini -> Groq -> OpenRouter ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    # 1) Gemini
    try:
        response = gemini_model.generate_content(prompt)
        return response.text, "Gemini"
    except Exception as e:
        gemini_err = str(e)

    # 2) Groq
    if groq_client:
        try:
            chat = groq_client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=600,
            )
            return chat.choices[0].message.content, "Groq"
        except Exception as e:
            groq_err = str(e)
    else:
        groq_err = "GROQ_API_KEY æœªè¨­å®š"

    # 3) OpenRouter
    if OPENROUTER_API_KEY:
        try:
            r = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://jstock-dashboard.streamlit.app",
                    "X-Title": "JStock Dashboard",
                },
                json={
                    "model": "meta-llama/llama-3.1-8b-instruct:free",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 600,
                },
                timeout=30,
            )
            r.raise_for_status()
            text = r.json()["choices"][0]["message"]["content"]
            return text, "OpenRouter"
        except Exception as e:
            or_err = str(e)
    else:
        or_err = "OPENROUTER_API_KEY æœªè¨­å®š"

    raise RuntimeError(
        f"å…¨AIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å¤±æ•— / Gemini: {gemini_err} / Groq: {groq_err} / OpenRouter: {or_err}"
    )

# ================================================================
# ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================

_NEWS_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

# â”€â”€ â‘  Yahoo!ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ Japan RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_yahoo_jp_news(ticker_code: str, max_items: int = 8) -> list[dict]:
    """
    Yahoo!ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ Japan ã®éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã‚’å–å¾—ã€‚
    ticker_code: '7203' ãªã©ï¼ˆ.T ãªã—ï¼‰
    """
    code = ticker_code.replace(".T", "")
    url = f"https://finance.yahoo.co.jp/rss/stocks/{code}"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title = item.findtext("title", "").strip()
            link  = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            desc  = item.findtext("description", "").strip()
            # HTMLã‚¿ã‚°é™¤å»
            desc = re.sub(r"<[^>]+>", "", desc)[:100]
            if title:
                items.append({"source": "Yahoo!Finance JP", "title": title,
                              "link": link, "date": pubdate, "summary": desc})
        return items
    except Exception:
        return []


# â”€â”€ â‘¡ æ ªæ¢ï¼ˆKabutanï¼‰éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_kabutan_news(ticker_code: str, max_items: int = 8) -> list[dict]:
    """
    æ ªæ¢ã®éŠ˜æŸ„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã€‚
    URL: https://kabutan.jp/stock/news?code=XXXX

    HTMLãƒ•ã‚¡ã‚¤ãƒ«å®Ÿæ¸¬ã«ã‚ˆã‚‹ç¢ºå®šæ§‹é€ :
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    <table class="s_news_list mgbt0">
      <tbody>
        <tr>
          <td class="news_time">
            <time datetime="2026-02-19T17:00:03+09:00">26/02/19&nbsp;17:00</time>
          </td>
          <td>
            <div class="newslist_ctg newsctg5_b">ç‰¹é›†</div>
          </td>
          <td>
            <a href="https://kabutan.jp/stock/news?code=5803&b=n202602191135">
              ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ—¥å ±ã€æœ€ä¸Šä½ã‚’ç¶™ç¶šï¼‹ç›®æ¨™æ ªä¾¡ã‚’å¢—é¡ã€‘(2æœˆ19æ—¥)
            </a>
          </td>
        </tr>
        ...
        <!-- é–‹ç¤ºï¼ˆPDFï¼‰ã®å ´åˆ -->
        <tr>
          <td class="news_time"><time ...>26/02/09&nbsp;14:00</time></td>
          <td><div class="newslist_ctg newsctg_kaiji_b">é–‹ç¤º</div></td>
          <td class="td_kaiji">
            <a href="https://kabutan.jp/disclosures/pdf/20260209/140120260206550334/" target="pdf">
              2026å¹´ï¼“æœˆæœŸé€šæœŸé€£çµæ¥­ç¸¾äºˆæƒ³...
            </a>
          </td>
        </tr>
      </tbody>
    </table>
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€» ã“ã®ãƒšãƒ¼ã‚¸è‡ªä½“ãŒ code=XXXX ã®éŠ˜æŸ„å°‚ç”¨ãƒšãƒ¼ã‚¸ãªã®ã§
      å–å¾—è¨˜äº‹ã¯ã™ã¹ã¦éŠ˜æŸ„å›ºæœ‰æƒ…å ±ã€‚
    â€» ãƒªãƒ³ã‚¯URLã« &b=n... (ãƒ‹ãƒ¥ãƒ¼ã‚¹) ã¾ãŸã¯ /disclosures/pdf/... (é–‹ç¤ºPDF) ã®2ç¨®é¡ã‚ã‚Šã€‚
    â€» ãƒ—ãƒ¬ãƒŸã‚¢ãƒ è¨˜äº‹ã¯ <img class="vat pdr4"> ãŒæŒ¿å…¥ã•ã‚Œã‚‹ã€‚
    """
    code = ticker_code.replace(".T", "")
    url = f"https://kabutan.jp/stock/news?code={code}"
    headers = {
        **_NEWS_HEADERS,
        "Accept": "text/html,application/xhtml+xml",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": "https://kabutan.jp/",
    }
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200:
            return []
        html = r.text

        # â”€â”€ s_news_list ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‚’å–å¾—
        table_match = re.search(
            r'class="s_news_list[^"]*"[^>]*>(.*?)</table>',
            html, re.DOTALL
        )
        if not table_match:
            return []
        table_html = table_match.group(1)

        # â”€â”€ tr è¡Œã”ã¨ã«ãƒ‘ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        rows = re.findall(r'<tr[^>]*>(.*?)</tr>', table_html, re.DOTALL)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šãƒãƒƒãƒ—
        ctg_class_map = {
            "newsctg2_b":    "ææ–™",
            "newsctg3_kk_b": "æ±ºç®—",
            "newsctg4_b":    "ãƒ†ã‚¯",
            "newsctg5_b":    "ç‰¹é›†",
            "newsctg_kaiji_b": "é–‹ç¤º",
        }
        badge_emoji = {
            "ææ–™": "ğŸŸ¢", "æ±ºç®—": "ğŸ”µ", "ãƒ†ã‚¯": "âšª",
            "ç‰¹é›†": "ğŸŸ ", "é–‹ç¤º": "ğŸ”´",
        }

        items = []
        for row in rows:
            # â‘  æ—¥æ™‚: <time datetime="2026-02-19T17:00:03+09:00">
            time_match = re.search(r'<time[^>]+datetime="([^"]+)"', row)
            if not time_match:
                continue
            # datetimeå±æ€§ã‹ã‚‰èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
            dt_raw = time_match.group(1)  # "2026-02-19T17:00:03+09:00"
            dt_disp = re.search(r'(\d{4}-\d{2}-\d{2})T(\d{2}:\d{2})', dt_raw)
            date_str = f"{dt_disp.group(1)} {dt_disp.group(2)}" if dt_disp else dt_raw[:16]

            # â‘¡ ã‚«ãƒ†ã‚´ãƒªãƒãƒƒã‚¸: class="newslist_ctg newsctgX_b"
            badge = ""
            for cls, label in ctg_class_map.items():
                if cls in row:
                    badge = label
                    break

            # â‘¢ ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«: 2ãƒ‘ã‚¿ãƒ¼ãƒ³
            #    a) ãƒ‹ãƒ¥ãƒ¼ã‚¹: href="https://kabutan.jp/stock/news?code=XXXX&b=nXXX"
            #    b) é–‹ç¤ºPDF:  href="https://kabutan.jp/disclosures/pdf/..."
            link_match = re.search(
                r'<a\s+href="(https://kabutan\.jp/(?:stock/news\?[^"]+|disclosures/pdf/[^"]+))"'
                r'[^>]*>\s*(.*?)\s*</a>',
                row, re.DOTALL
            )
            if not link_match:
                continue

            link = link_match.group(1).replace("&amp;", "&")
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰HTMLã‚¿ã‚°ï¼ˆimgãªã©ï¼‰ã‚’é™¤å»
            title = re.sub(r'<[^>]+>', '', link_match.group(2)).strip()

            if len(title) < 3:
                continue

            # â‘£ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ è¨˜äº‹ã®æ¤œå‡ºï¼ˆãƒ­ãƒƒã‚¯ç”»åƒãŒæŒ¿å…¥ã•ã‚Œã‚‹ï¼‰
            is_premium = "ğŸ”’ " if "premium" in row.lower() or "pdr4" in row else ""

            emoji = badge_emoji.get(badge, "ğŸ“°")

            items.append({
                "source": "æ ªæ¢(Kabutan)",
                "title": f"{is_premium}{title}",
                "badge": badge,
                "badge_emoji": emoji,
                "link": link,
                "date": date_str,
                "summary": "",
                "ticker_specific": True,
            })

            if len(items) >= max_items:
                break

        return items

    except Exception:
        return []


# â”€â”€ è‹±èªç¤¾åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆä¸»è¦éŠ˜æŸ„ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_JP_EN_NAME_MAP = {
    "ãƒˆãƒ¨ã‚¿": "Toyota", "ãƒ›ãƒ³ãƒ€": "Honda", "æ—¥ç”£è‡ª": "Nissan", "ã‚½ãƒ‹ãƒ¼ï¼§": "Sony",
    "ä¸‰è±ï¼µï¼¦ï¼ª": "Mitsubishi UFJ", "ä¸‰äº•ä½å‹ï¼¦ï¼§": "Sumitomo Mitsui",
    "ã¿ãšã»ï¼¦ï¼§": "Mizuho", "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯": "SoftBank", "ï¼³ï¼¢ï¼§": "SoftBank",
    "ä»»å¤©å ‚": "Nintendo", "ãƒ‘ãƒŠï¼¨ï¼¤": "Panasonic", "æ—¥ç«‹": "Hitachi",
    "å¯Œå£«é€š": "Fujitsu", "ï¼®ï¼¥ï¼£": "NEC", "ã‚­ãƒ¤ãƒãƒ³": "Canon",
    "ã‚·ãƒ£ãƒ¼ãƒ—": "Sharp", "æ±ã‚¨ãƒ¬ã‚¯": "Tokyo Electron", "ä¿¡è¶ŠåŒ–": "Shin-Etsu",
    "æ‘ç”°è£½": "Murata", "äº¬ã‚»ãƒ©": "Kyocera", "ãƒ€ã‚¤ã‚­ãƒ³": "Daikin",
    "ã‚³ãƒãƒ„": "Komatsu", "ãƒ•ã‚¡ãƒŠãƒƒã‚¯": "Fanuc", "ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹": "Keyence",
    "ãƒ«ãƒã‚µã‚¹": "Renesas", "ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ": "Advantest", "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¯": "Lasertec",
    "ãƒ‡ã‚£ã‚¹ã‚³": "Disco", "ãƒ‹ãƒ‡ãƒƒã‚¯": "Nidec", "ä¸‰è±é›»": "Mitsubishi Electric",
    "ä¼Šè—¤å¿ ": "Itochu", "ä¸‰è±å•†": "Mitsubishi Corp", "ä¸‰äº•ç‰©": "Mitsui",
    "ä½å‹å•†": "Sumitomo Corp", "ä¸¸ç´…": "Marubeni", "æ­¦ç”°": "Takeda",
    "ã‚¨ãƒ¼ã‚¶ã‚¤": "Eisai", "ç¬¬ä¸€ä¸‰å…±": "Daiichi Sankyo", "ä¸­å¤–è–¬": "Chugai",
    "ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹": "Astellas", "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ": "Recruit", "ãƒ¡ãƒ«ã‚«ãƒª": "Mercari",
    "æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—": "Rakuten", "ï¼®ï¼´ï¼´": "NTT", "ï¼«ï¼¤ï¼¤ï¼©": "KDDI",
    "æ±äº¬æµ·ä¸Š": "Tokio Marine", "ï¼ªï¼´": "Japan Tobacco",
    "æ—¥æœ¬è£½é‰„": "Nippon Steel", "ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³": "Bridgestone",
    "ï¼ªï¼¡ï¼¬": "Japan Airlines", "ï¼¡ï¼®ï¼¡ï¼¨ï¼¤": "ANA",
}

def _get_en_name(company_name: str) -> str:
    """æ—¥æœ¬èªç¤¾åã‹ã‚‰è‹±èªåã‚’æ¨å®š"""
    for jp, en in _JP_EN_NAME_MAP.items():
        if jp in company_name:
            return en
    # ãƒ­ãƒ¼ãƒå­—ã£ã½ã„æ–‡å­—åˆ—ã‚’å«ã‚€å ´åˆã¯ãã®ã¾ã¾
    ascii_part = re.sub(r'[^\x20-\x7E]', '', company_name).strip()
    return ascii_part if len(ascii_part) >= 2 else ""


# â”€â”€ Google News RSSï¼ˆéå»90æ—¥ï¼‰éŠ˜æŸ„æ¤œç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def _fetch_google_news_rss(query: str, source_filter: str, max_items: int, days: int = 90) -> list[dict]:
    """
    Google News RSS ã§ query ã‚’æ¤œç´¢ã—ã€æŒ‡å®šã‚½ãƒ¼ã‚¹ã®è¨˜äº‹ã®ã¿è¿”ã™ã€‚
    days: éå»ä½•æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿è¿”ã™ã‹
    """
    import datetime as dt
    import urllib.parse

    q_enc = urllib.parse.quote(query)
    url = f"https://news.google.com/rss/search?q={q_enc}&hl=ja&gl=JP&ceid=JP:ja"
    cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=days)

    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=15)
        if r.status_code != 200:
            return []
        # Google News RSSã¯UTF-8
        content = r.content
        # namespaceå®£è¨€ãŒå£Šã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§å‰å‡¦ç†
        content = re.sub(rb'<\?xml[^?]*\?>', b'<?xml version="1.0" encoding="UTF-8"?>', content)
        root = ET.fromstring(content)
        items = []
        for item in root.findall(".//item"):
            title   = item.findtext("title", "").strip()
            link    = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            source_elem = item.find("source")
            source_name = source_elem.text.strip() if source_elem is not None else ""

            if not title:
                continue

            # ã‚½ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
            if source_filter and source_filter.lower() not in source_name.lower():
                continue

            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéå»daysæ—¥ä»¥å†…ï¼‰
            if pubdate:
                try:
                    from email.utils import parsedate_to_datetime
                    pub_dt = parsedate_to_datetime(pubdate)
                    if pub_dt.tzinfo is None:
                        import datetime as dt2
                        pub_dt = pub_dt.replace(tzinfo=dt2.timezone.utc)
                    if pub_dt < cutoff:
                        continue
                    date_str = pub_dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    date_str = pubdate[:16]
            else:
                date_str = ""

            items.append({
                "title": title,
                "link": link,
                "date": date_str,
                "source_name": source_name,
            })
            if len(items) >= max_items:
                break
        return items
    except Exception:
        return []


# â”€â”€ â‘¢ æ—¥çµŒæ–°è éŠ˜æŸ„åˆ¥ï¼ˆGoogle NewsçµŒç”±ãƒ»éå»90æ—¥ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_nikkei_stock_news(company_name: str, ticker_code: str, max_items: int = 10) -> list[dict]:
    """æ—¥çµŒæ–°èã®éŠ˜æŸ„é–¢é€£è¨˜äº‹ã‚’Google News RSSçµŒç”±ã§å–å¾—ï¼ˆéå»90æ—¥ï¼‰"""
    code = ticker_code.replace(".T", "")
    en_name = _get_en_name(company_name)
    company_short = re.sub(r'[ã€€ï¼ˆï¼‰()ï¼¨ï¼¤ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹\s]', '', company_name)

    # è¤‡æ•°ã‚¯ã‚¨ãƒªã‚’è©¦ã—ã¦ãƒãƒ¼ã‚¸
    queries = [f"{company_short} site:nikkei.com", f"{code} æ—¥çµŒ"]
    if en_name:
        queries.append(f"{en_name} nikkei")

    all_items = []
    seen = set()
    for q in queries:
        for it in _fetch_google_news_rss(q, "æ—¥çµŒ", max_items * 2, days=90):
            k = it["title"][:40]
            if k not in seen:
                seen.add(k)
                all_items.append({
                    "source": "æ—¥çµŒæ–°è",
                    "title": it["title"],
                    "link": it["link"],
                    "date": it["date"],
                    "summary": "",
                    "ticker_specific": True,
                })
        if len(all_items) >= max_items:
            break
    return all_items[:max_items]


# â”€â”€ â‘£ CNBC éŠ˜æŸ„åˆ¥ï¼ˆGoogle NewsçµŒç”±ãƒ»éå»90æ—¥ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_cnbc_news(company_name: str, ticker_code: str, max_items: int = 10) -> list[dict]:
    """CNBCã®éŠ˜æŸ„é–¢é€£è¨˜äº‹ã‚’Google News RSSçµŒç”±ã§å–å¾—ï¼ˆéå»90æ—¥ï¼‰"""
    code = ticker_code.replace(".T", "")
    en_name = _get_en_name(company_name)

    queries = []
    if en_name:
        queries.append(f"{en_name} site:cnbc.com")
        queries.append(f"{en_name} CNBC")
    queries.append(f"{code} CNBC")

    all_items = []
    seen = set()
    for q in queries:
        for it in _fetch_google_news_rss(q, "CNBC", max_items * 2, days=90):
            k = it["title"][:40]
            if k not in seen:
                seen.add(k)
                all_items.append({
                    "source": "CNBC",
                    "title": it["title"],
                    "link": it["link"],
                    "date": it["date"],
                    "summary": "",
                    "ticker_specific": True,
                })
        if len(all_items) >= max_items:
            break
    return all_items[:max_items]


# â”€â”€ â‘£ TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰éŠ˜æŸ„åˆ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=3600)
def fetch_tdnet_news(ticker_code: str, max_items: int = 20, months: int = 3) -> list[dict]:
    """
    æ ªæ¢ã®é–‹ç¤ºã‚¿ãƒ–ï¼ˆnmode=3ï¼‰ã‹ã‚‰éå»N ãƒ¶æœˆåˆ†ã®é©æ™‚é–‹ç¤ºã‚’å–å¾—ã€‚

    â–¼ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸å®šã®æ ¹æ‹ 
      TDnetæœ¬å®¶ (release.tdnet.info) ã¯æ—¥ä»˜é¸æŠå¼ã§éå»ç´„1ãƒ¶æœˆåˆ†ã®ã¿ã€‚
      æ ªæ¢ã®é–‹ç¤ºã‚¿ãƒ– (kabutan.jp/stock/news?code=XXXX&nmode=3) ã¯
      è¤‡æ•°ãƒšãƒ¼ã‚¸ã§æ•°å¹´åˆ†ã¾ã§é¡ã‚Œã‚‹ãŸã‚ã€ã“ã¡ã‚‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

    â–¼ å–å¾—æˆ¦ç•¥
      - æ–°ã—ã„é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã§ page=1 ã‹ã‚‰é †ã«ãŸã©ã‚‹
      - å„è¡Œã® datetime ã‚’è¦‹ã¦ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚ˆã‚Šå¤ããªã£ãŸã‚‰çµ‚äº†
      - PDFãƒªãƒ³ã‚¯ã¯ kabutan.jp/disclosures/pdf/... å½¢å¼
    """
    import datetime as dt
    code = ticker_code.replace(".T", "")
    cutoff = dt.datetime.now() - dt.timedelta(days=months * 31)
    base_headers = {
        **_NEWS_HEADERS,
        "Accept": "text/html,application/xhtml+xml",
        "Accept-Language": "ja,en-US;q=0.9",
        "Referer": "https://kabutan.jp/",
    }
    items = []
    page = 1

    while len(items) < max_items and page <= 10:  # æœ€å¤§10ãƒšãƒ¼ã‚¸
        url = f"https://kabutan.jp/stock/news?code={code}&nmode=3&page={page}"
        try:
            r = requests.get(url, headers=base_headers, timeout=15)
            if r.status_code != 200:
                break
            html = r.text

            table_match = re.search(
                r'class="s_news_list[^"]*"[^>]*>(.*?)</table>',
                html, re.DOTALL
            )
            if not table_match:
                break
            table_html = table_match.group(1)
            rows = re.findall(r'<tr[^>]*>(.*?)</tr>', table_html, re.DOTALL)
            if not rows:
                break

            found_on_page = 0
            hit_cutoff = False

            for row in rows:
                time_match = re.search(r'<time[^>]+datetime="([^"]+)"', row)
                if not time_match:
                    continue
                dt_raw = time_match.group(1)
                dt_disp = re.search(r'(\d{4}-\d{2}-\d{2})T(\d{2}:\d{2})', dt_raw)
                if not dt_disp:
                    continue
                date_str = f"{dt_disp.group(1)} {dt_disp.group(2)}"

                # ã‚«ãƒƒãƒˆã‚ªãƒ•ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°ã—ã„é †ãªã®ã§ã“ã“ä»¥é™ã¯å…¨éƒ¨å¤ã„ï¼‰
                try:
                    row_dt = dt.datetime.strptime(date_str, "%Y-%m-%d %H:%M")
                    if row_dt < cutoff:
                        hit_cutoff = True
                        break
                except Exception:
                    pass

                link_match = re.search(
                    r'<a\s+href="(https://kabutan\.jp/disclosures/[^"]+)"[^>]*>\s*(.*?)\s*</a>',
                    row, re.DOTALL
                )
                if not link_match:
                    continue
                link  = link_match.group(1)
                title = re.sub(r'<[^>]+>', '', link_match.group(2)).strip()
                if len(title) < 3:
                    continue

                items.append({
                    "source": "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰",
                    "title": title,
                    "badge": "é–‹ç¤º",
                    "badge_emoji": "ğŸ”´",
                    "link": link,
                    "date": date_str,
                    "summary": "ğŸ“„ é©æ™‚é–‹ç¤ºPDF",
                    "ticker_specific": True,
                })
                found_on_page += 1
                if len(items) >= max_items:
                    return items

            if hit_cutoff or found_on_page == 0:
                break
            page += 1

        except Exception:
            break

    return items


@st.cache_data(ttl=7200)
def ai_summarize_tdnet_pdf(pdf_url: str, title: str) -> str:
    """
    é©æ™‚é–‹ç¤ºã®å†…å®¹ã‚’AIã§è©³ç´°è¦ç´„ã€‚
    æ ªæ¢ã®é–‹ç¤ºHTMLãƒšãƒ¼ã‚¸å–å¾— -> PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º -> ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    """
    page_text = ""
    source_desc = ""

    # 1) æ ªæ¢ã®é–‹ç¤ºHTMLãƒšãƒ¼ã‚¸ï¼ˆ/disclosures/pdf/ -> /disclosures/ ã«å¤‰æ›ï¼‰
    try:
        html_url = pdf_url.replace("/disclosures/pdf/", "/disclosures/")
        if html_url != pdf_url:
            r = requests.get(html_url, headers={**_NEWS_HEADERS, "Referer": "https://kabutan.jp/"}, timeout=15)
            if r.status_code == 200 and "text/html" in r.headers.get("Content-Type", ""):
                raw = re.sub(r'<script[^>]*>.*?</script>', ' ', r.text, flags=re.DOTALL)
                raw = re.sub(r'<style[^>]*>.*?</style>', ' ', raw, flags=re.DOTALL)
                raw = re.sub(r'<[^>]+>', ' ', raw)
                raw = re.sub(r'\s+', ' ', raw).strip()
                page_text = raw[:6000]
                source_desc = "æ ªæ¢é–‹ç¤ºãƒšãƒ¼ã‚¸"
    except Exception:
        pass

    # 2) PDFç›´æ¥å–å¾—ï¼ˆãƒã‚¤ãƒŠãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡ºï¼‰
    if not page_text:
        try:
            r = requests.get(pdf_url, headers={**_NEWS_HEADERS, "Referer": "https://kabutan.jp/"}, timeout=20)
            if r.status_code == 200:
                ct = r.headers.get("Content-Type", "")
                if "text/html" in ct:
                    raw = re.sub(r'<[^>]+>', ' ', r.text)
                    page_text = re.sub(r'\s+', ' ', raw).strip()[:6000]
                    source_desc = "é–‹ç¤ºHTML"
                elif "pdf" in ct.lower():
                    pdf_str = r.content.decode("latin-1", errors="ignore")
                    chunks = re.findall(r'BT\s*(.*?)\s*ET', pdf_str, re.DOTALL)
                    parts = []
                    for chunk in chunks:
                        parts.extend(re.findall(r'\(([^)]{1,200})\)', chunk))
                    page_text = " ".join(parts)[:6000]
                    source_desc = "PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"
        except Exception:
            pass

    if not page_text or len(page_text.strip()) < 50:
        page_text = "ï¼ˆæœ¬æ–‡å–å¾—ä¸å¯ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨å®šï¼‰"
        source_desc = "ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿"

    prompt = f"""ã‚ãªãŸã¯æ—¥æœ¬æ ªã®æ©Ÿé–¢æŠ•è³‡å®¶å‘ã‘ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’åˆ†æã—ã€æŠ•è³‡åˆ¤æ–­ã«å½¹ç«‹ã¤è©³ç´°ãªè¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}

ã€é–‹ç¤ºå†…å®¹ã€‘{page_text}

ã€è¦ç´„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆåˆè¨ˆ400ã€œ500æ–‡å­—ï¼‰ã€‘

â–  é–‹ç¤ºç¨®åˆ¥: ï¼ˆæ¥­ç¸¾ä¿®æ­£ / é…å½“å¤‰æ›´ / æ±ºç®—ç™ºè¡¨ / è³‡æœ¬æ”¿ç­– / ãã®ä»–ï¼‰

â–  ä¸»è¦ãªå¤‰æ›´ç‚¹:
  - å¤‰æ›´å‰â†’å¤‰æ›´å¾Œã®æ•°å€¤ã‚’å…·ä½“çš„ã«ï¼ˆä¾‹: å–¶æ¥­åˆ©ç›Š 500å„„å††â†’620å„„å††ã€+24%ï¼‰
  - é…å½“ãŒã‚ã‚‹å ´åˆã¯1æ ªã‚ãŸã‚Šã®é‡‘é¡ã‚‚è¨˜è¼‰
  - è¤‡æ•°é …ç›®ã‚ã‚‹å ´åˆã¯ã™ã¹ã¦åˆ—æŒ™

â–  èƒŒæ™¯ãƒ»ç†ç”±: ãªãœä¿®æ­£ãƒ»ç™ºè¡¨ã—ãŸã‹

â–  æ ªä¾¡ã¸ã®å½±éŸ¿äºˆæ¸¬:
  - ãƒã‚¸ãƒ†ã‚£ãƒ– / ãƒã‚¬ãƒ†ã‚£ãƒ– / ä¸­ç«‹ ã¨ãã®ç†ç”±
  - å¸‚å ´ãŒæ³¨ç›®ã™ã¹ããƒã‚¤ãƒ³ãƒˆ

â–  æŠ•è³‡å®¶ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹: çŸ­æœŸãƒ»ä¸­é•·æœŸã®è¦³ç‚¹ã§
"""
    try:
        comment, ai_name = generate_ai_comment(prompt)
        return comment + "\n\n_æƒ…å ±æº: " + source_desc + " / AI: " + ai_name + "_"
    except Exception as e:
        return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}"




# â”€â”€ â‘¤ Reuters éŠ˜æŸ„åˆ¥ï¼ˆGoogle NewsçµŒç”±ãƒ»éå»90æ—¥ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_reuters_stock_news(company_name: str, ticker_code: str, max_items: int = 10) -> list[dict]:
    """Reutersã®éŠ˜æŸ„é–¢é€£è¨˜äº‹ã‚’Google News RSSçµŒç”±ã§å–å¾—ï¼ˆéå»90æ—¥ï¼‰"""
    code = ticker_code.replace(".T", "")
    en_name = _get_en_name(company_name)
    company_short = re.sub(r'[ã€€ï¼ˆï¼‰()ï¼¨ï¼¤ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹\s]', '', company_name)

    queries = []
    if en_name:
        queries.append(f"{en_name} site:reuters.com")
        queries.append(f"{en_name} Reuters")
    queries.append(f"{company_short} ãƒ­ã‚¤ã‚¿ãƒ¼")
    queries.append(f"{code} Reuters")

    all_items = []
    seen = set()
    for q in queries:
        for it in _fetch_google_news_rss(q, "Reuters", max_items * 2, days=90):
            k = it["title"][:40]
            if k not in seen:
                seen.add(k)
                all_items.append({
                    "source": "Reuters JP",
                    "title": it["title"],
                    "link": it["link"],
                    "date": it["date"],
                    "summary": "",
                    "ticker_specific": True,
                })
        if len(all_items) >= max_items:
            break
    return all_items[:max_items]


# â”€â”€ â‘¥ æ—¥çµŒæ–°è ãƒãƒ¼ã‚±ãƒƒãƒˆ RSSï¼ˆå…¨ä½“ãƒ»Tab3ç”¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_nikkei_market_rss(max_items: int = 8) -> list[dict]:
    """æ—¥çµŒæ–°èãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹ RSSï¼ˆå…¨ä½“å¸‚æ³ï¼‰"""
    url = "https://www.nikkei.com/rss/market.xml"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title   = item.findtext("title", "").strip()
            link    = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            if title:
                items.append({"source": "æ—¥çµŒæ–°è", "title": title,
                              "link": link, "date": pubdate, "summary": ""})
        return items
    except Exception:
        return []


# â”€â”€ â‘¦ Reuters Japan RSSï¼ˆå…¨ä½“ãƒ»Tab3ç”¨ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_reuters_jp_rss(max_items: int = 8) -> list[dict]:
    """Reutersæ—¥æœ¬èªãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹"""
    url = "https://feeds.reuters.com/reuters/JPBusinessNews"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title   = item.findtext("title", "").strip()
            link    = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            if title:
                items.append({"source": "Reuters JP", "title": title,
                              "link": link, "date": pubdate, "summary": ""})
        return items
    except Exception:
        return []


# â”€â”€ â‘¦ ã‚½ãƒ¼ã‚¹åˆ¥ä¸¦åˆ—å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_news_by_source(
    ticker_code: str,
    company_name: str,
    max_per_source: int = 10,
) -> dict:
    """
    å„ã‚½ãƒ¼ã‚¹ã‚’ä¸¦åˆ—å–å¾—ã—ã€ã‚½ãƒ¼ã‚¹åã‚’ã‚­ãƒ¼ã«ã—ãŸè¾æ›¸ã§è¿”ã™ã€‚
    {
      "Yahoo!Finance JP": [...],
      "æ ªæ¢(Kabutan)":    [...],
      "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰": [...],
      "æ—¥çµŒæ–°è":          [...],
      "CNBC":             [...],
      "Reuters JP":       [...],
    }
    å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥çµŒãƒ»CNBCãƒ»Reutersï¼‰ã¯éŠ˜æŸ„åãƒ»ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¨˜äº‹ã®ã¿æ®‹ã™ã€‚
    """
    import concurrent.futures
    code = ticker_code.replace(".T", "")

    # éŠ˜æŸ„ãƒãƒƒãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    company_short = re.sub(r"[ã€€ï¼¨ï¼¤ï¼ˆï¼‰()ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹]", "", company_name)[:6]
    keywords = {code, company_name, company_short,
                company_name[:4], company_name.replace("ï¼¨ï¼¤", "").strip()}
    keywords = {k for k in keywords if len(k) >= 2}

    tasks = {
        "Yahoo!Finance JP":  lambda: fetch_yahoo_jp_news(code, max_per_source),
        "æ ªæ¢(Kabutan)":     lambda: fetch_kabutan_news(code, max_per_source),
        "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰": lambda: fetch_tdnet_news(code, max_items=30, months=3),
        "æ—¥çµŒæ–°è":          lambda: fetch_nikkei_stock_news(company_name, code, max_per_source),
        "CNBC":              lambda: fetch_cnbc_news(company_name, code, max_per_source),
        "Reuters JP":        lambda: fetch_reuters_stock_news(company_name, code, max_per_source),
    }

    results_by_source = {k: [] for k in tasks}

    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as ex:
        futures = {ex.submit(fn): key for key, fn in tasks.items()}
        for future in concurrent.futures.as_completed(futures):
            key = futures[future]
            try:
                items = future.result()
                results_by_source[key] = items
            except Exception:
                results_by_source[key] = []

    return results_by_source


# â”€â”€ â‘§ AI ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ai_news_summary(news_items, company_name: str, ticker: str) -> str:
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’AIã§æ—¥æœ¬èªè¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æã€‚
    news_items: list[dict] ã¾ãŸã¯ list[str]ï¼ˆè¦‹å‡ºã—æ–‡å­—åˆ—ï¼‰ã‚’å—ã‘ä»˜ã‘ã‚‹ã€‚
    """
    if not news_items:
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    if isinstance(news_items[0], str):
        headlines = "\n".join(news_items[:20])
    else:
        headlines = "\n".join(
            f"[{it['source']}] {it['title']}" for it in news_items[:20]
        )
    prompt = f"""
ä»¥ä¸‹ã¯æ—¥æœ¬æ ªã€Œ{company_name}ï¼ˆ{ticker}ï¼‰ã€ã«é–¢ã™ã‚‹æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»é©æ™‚é–‹ç¤ºã®è¦‹å‡ºã—ã§ã™ã€‚

{headlines}

æŠ•è³‡å®¶å‘ã‘ã«ä»¥ä¸‹ã‚’æ—¥æœ¬èª300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
1. ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ¤å®š: ã€å¼·æ°— / å¼±æ°— / ä¸­ç«‹ã€‘
2. æ³¨ç›®ã‚¤ãƒ™ãƒ³ãƒˆã®è¦ç‚¹
3. æ ªä¾¡ã¸ã®å½±éŸ¿ã®å¯èƒ½æ€§
"""
    try:
        comment, ai_name = generate_ai_comment(prompt)
        return f"{comment}\n\n_AI: {ai_name}_"
    except Exception as e:
        return f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"


# ================================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼
# ================================================================
with st.sidebar:
    st.header("âš™ï¸ åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    years          = st.number_input("ğŸ“… éå»ä½•å¹´ã§åˆ†æï¼Ÿ", 1, 10, 3)
    risk_free_rate = st.number_input("ğŸ“‰ ç„¡ãƒªã‚¹ã‚¯é‡‘åˆ©ï¼ˆ%ï¼‰", 0.0, 10.0, 1.0, step=0.1) / 100
    top_n          = st.number_input("ğŸ“Š ä¸Šä½ä½•ç¤¾ã‚’è¡¨ç¤ºï¼Ÿ", 5, 50, 20, step=5)
    st.divider()

    st.header("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨­å®š")
    news_max_per_source = st.slider("å„ã‚½ãƒ¼ã‚¹ã®æœ€å¤§å–å¾—ä»¶æ•°", 3, 10, 5)
    show_news_sources = st.multiselect(
        "è¡¨ç¤ºã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹",
        ["Yahoo!Finance JP", "æ ªæ¢(Kabutan)", "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰", "æ—¥çµŒæ–°è", "CNBC", "Reuters JP"],
        default=["Yahoo!Finance JP", "æ ªæ¢(Kabutan)", "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰", "æ—¥çµŒæ–°è", "CNBC", "Reuters JP"],
    )
    st.divider()
    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: Yahoo Finance, TDnet, æ ªæ¢, ã¿ã‚“ã‹ã¶, æ—¥çµŒ, Reuters")

# ================================================================
# éŠ˜æŸ„ãƒã‚¹ã‚¿
# ================================================================
ticker_name_map = {
    '1332.T': ('ãƒ‹ãƒƒã‚¹ã‚¤', 'æ°´ç”£'),
    '1605.T': ('ï¼©ï¼®ï¼°ï¼¥ï¼¸', 'é‰±æ¥­'),
    '1721.T': ('ã‚³ãƒ ã‚·ã‚¹ï¼¨ï¼¤', 'å»ºè¨­'),
    '1801.T': ('å¤§æˆå»º', 'å»ºè¨­'),
    '1802.T': ('å¤§æ—çµ„', 'å»ºè¨­'),
    '1803.T': ('æ¸…æ°´å»º', 'å»ºè¨­'),
    '1808.T': ('é•·è°·å·¥', 'å»ºè¨­'),
    '1812.T': ('é¹¿å³¶', 'å»ºè¨­'),
    '1925.T': ('ãƒã‚¦ã‚¹', 'å»ºè¨­'),
    '1928.T': ('ç©ãƒã‚¦ã‚¹', 'å»ºè¨­'),
    '1963.T': ('æ—¥æ®ï¼¨ï¼¤', 'å»ºè¨­'),
    '2002.T': ('æ—¥æ¸…ç²‰ï¼§', 'é£Ÿå“'),
    '2269.T': ('æ˜æ²»ï¼¨ï¼¤', 'é£Ÿå“'),
    '2282.T': ('æ—¥ãƒãƒ ', 'é£Ÿå“'),
    '2413.T': ('ã‚¨ãƒ ã‚¹ãƒªãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '2432.T': ('ãƒ‡ã‚£ãƒ¼ã‚¨ãƒŒã‚¨', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '2501.T': ('ã‚µãƒƒãƒãƒ­ï¼¨ï¼¤', 'é£Ÿå“'),
    '2502.T': ('ã‚¢ã‚µãƒ’', 'é£Ÿå“'),
    '2503.T': ('ã‚­ãƒªãƒ³ï¼¨ï¼¤', 'é£Ÿå“'),
    '2768.T': ('åŒæ—¥', 'å•†ç¤¾'),
    '2801.T': ('ã‚­ãƒƒã‚³ãƒãƒ³', 'é£Ÿå“'),
    '2802.T': ('å‘³ã®ç´ ', 'é£Ÿå“'),
    '2871.T': ('ãƒ‹ãƒãƒ¬ã‚¤', 'é£Ÿå“'),
    '2914.T': ('ï¼ªï¼´', 'é£Ÿå“'),
    '3086.T': ('ï¼ªãƒ•ãƒ­ãƒ³ãƒˆ', 'å°å£²æ¥­'),
    '3092.T': ('ï¼ºï¼¯ï¼ºï¼¯', 'å°å£²æ¥­'),
    '3099.T': ('ä¸‰è¶Šä¼Šå‹¢ä¸¹', 'å°å£²æ¥­'),
    '3289.T': ('æ±æ€¥ä¸ï¼¨ï¼¤', 'ä¸å‹•ç”£'),
    '3382.T': ('ã‚»ãƒ–ãƒ³ï¼†ã‚¢ã‚¤', 'å°å£²æ¥­'),
    '3401.T': ('å¸äºº', 'ç¹Šç¶­'),
    '3402.T': ('æ±ãƒ¬', 'ç¹Šç¶­'),
    '3405.T': ('ã‚¯ãƒ©ãƒ¬', 'åŒ–å­¦'),
    '3407.T': ('æ—­åŒ–æˆ', 'åŒ–å­¦'),
    '3436.T': ('ï¼³ï¼µï¼­ï¼£ï¼¯', 'éé‰„ãƒ»é‡‘å±'),
    '3659.T': ('ãƒã‚¯ã‚½ãƒ³', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '3861.T': ('ç‹å­ï¼¨ï¼¤', 'ãƒ‘ãƒ«ãƒ—ãƒ»ç´™'),
    '4004.T': ('ãƒ¬ã‚¾ãƒŠãƒƒã‚¯', 'åŒ–å­¦'),
    '4005.T': ('ä½å‹åŒ–', 'åŒ–å­¦'),
    '4021.T': ('æ—¥ç”£åŒ–', 'åŒ–å­¦'),
    '4042.T': ('æ±ã‚½ãƒ¼', 'åŒ–å­¦'),
    '4043.T': ('ãƒˆã‚¯ãƒ¤ãƒ', 'åŒ–å­¦'),
    '4061.T': ('ãƒ‡ãƒ³ã‚«', 'åŒ–å­¦'),
    '4063.T': ('ä¿¡è¶ŠåŒ–', 'åŒ–å­¦'),
    '4151.T': ('å”å’Œã‚­ãƒªãƒ³', 'åŒ»è–¬å“'),
    '4183.T': ('ä¸‰äº•åŒ–å­¦', 'åŒ–å­¦'),
    '4188.T': ('ä¸‰è±ã‚±ãƒŸï¼§', 'åŒ–å­¦'),
    '4208.T': ('ï¼µï¼¢ï¼¥', 'åŒ–å­¦'),
    '4307.T': ('é‡æ‘ç·ç ”', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4324.T': ('é›»é€šã‚°ãƒ«ãƒ¼ãƒ—', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4385.T': ('ãƒ¡ãƒ«ã‚«ãƒª', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4452.T': ('èŠ±ç‹', 'åŒ–å­¦'),
    '4502.T': ('æ­¦ç”°', 'åŒ»è–¬å“'),
    '4503.T': ('ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹', 'åŒ»è–¬å“'),
    '4506.T': ('ä½å‹ãƒ•ã‚¡ãƒ¼ãƒ', 'åŒ»è–¬å“'),
    '4507.T': ('å¡©é‡ç¾©', 'åŒ»è–¬å“'),
    '4519.T': ('ä¸­å¤–è–¬', 'åŒ»è–¬å“'),
    '4523.T': ('ã‚¨ãƒ¼ã‚¶ã‚¤', 'åŒ»è–¬å“'),
    '4543.T': ('ãƒ†ãƒ«ãƒ¢', 'ç²¾å¯†æ©Ÿå™¨'),
    '4568.T': ('ç¬¬ä¸€ä¸‰å…±', 'åŒ»è–¬å“'),
    '4578.T': ('å¤§å¡šï¼¨ï¼¤', 'åŒ»è–¬å“'),
    '4661.T': ('ï¼¯ï¼¬ï¼£', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4689.T': ('ãƒ©ã‚¤ãƒ³ãƒ¤ãƒ•ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4704.T': ('ãƒˆãƒ¬ãƒ³ãƒ‰', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4751.T': ('ã‚µã‚¤ãƒãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4755.T': ('æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4901.T': ('å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ', 'åŒ–å­¦'),
    '4902.T': ('ã‚³ãƒ‹ã‚«ãƒŸãƒãƒ«', 'ç²¾å¯†æ©Ÿå™¨'),
    '4911.T': ('è³‡ç”Ÿå ‚', 'åŒ–å­¦'),
    '5019.T': ('å‡ºå…‰èˆˆç”£', 'çŸ³æ²¹'),
    '5020.T': ('ï¼¥ï¼®ï¼¥ï¼¯ï¼³', 'çŸ³æ²¹'),
    '5101.T': ('æµœã‚´ãƒ ', 'ã‚´ãƒ '),
    '5108.T': ('ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³', 'ã‚´ãƒ '),
    '5201.T': ('ï¼¡ï¼§ï¼£', 'çª¯æ¥­'),
    '5214.T': ('æ—¥é›»ç¡', 'çª¯æ¥­'),
    '5233.T': ('å¤ªå¹³æ´‹ã‚»ãƒ¡', 'çª¯æ¥­'),
    '5301.T': ('æ±æµ·ã‚«ãƒ¼ãƒœãƒ³', 'çª¯æ¥­'),
    '5332.T': ('ï¼´ï¼¯ï¼´ï¼¯', 'çª¯æ¥­'),
    '5333.T': ('ã‚¬ã‚¤ã‚·', 'çª¯æ¥­'),
    '5401.T': ('æ—¥æœ¬è£½é‰„', 'é‰„é‹¼'),
    '5406.T': ('ç¥æˆ¸é‹¼', 'é‰„é‹¼'),
    '5411.T': ('ï¼ªï¼¦ï¼¥', 'é‰„é‹¼'),
    '5631.T': ('æ—¥è£½é‹¼', 'æ©Ÿæ¢°'),
    '5706.T': ('ä¸‰äº•é‡‘', 'éé‰„ãƒ»é‡‘å±'),
    '5711.T': ('ä¸‰è±ãƒ', 'éé‰„ãƒ»é‡‘å±'),
    '5713.T': ('ä½å‹é‰±', 'éé‰„ãƒ»é‡‘å±'),
    '5714.T': ('ï¼¤ï¼¯ï¼·ï¼¡', 'éé‰„ãƒ»é‡‘å±'),
    '5801.T': ('å¤æ²³é›»', 'éé‰„ãƒ»é‡‘å±'),
    '5802.T': ('ä½å‹é›»', 'éé‰„ãƒ»é‡‘å±'),
    '5803.T': ('ãƒ•ã‚¸ã‚¯ãƒ©', 'éé‰„ãƒ»é‡‘å±'),
    '5831.T': ('ã—ãšãŠã‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '6098.T': ('ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6103.T': ('ã‚ªãƒ¼ã‚¯ãƒ', 'æ©Ÿæ¢°'),
    '6113.T': ('ã‚¢ãƒãƒ€', 'æ©Ÿæ¢°'),
    '6146.T': ('ãƒ‡ã‚£ã‚¹ã‚³', 'ç²¾å¯†æ©Ÿå™¨'),
    '6178.T': ('æ—¥æœ¬éƒµæ”¿', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6273.T': ('ï¼³ï¼­ï¼£', 'æ©Ÿæ¢°'),
    '6301.T': ('ã‚³ãƒãƒ„', 'æ©Ÿæ¢°'),
    '6302.T': ('ä½å‹é‡', 'æ©Ÿæ¢°'),
    '6305.T': ('æ—¥ç«‹å»ºæ©Ÿ', 'æ©Ÿæ¢°'),
    '6326.T': ('ã‚¯ãƒœã‚¿', 'æ©Ÿæ¢°'),
    '6361.T': ('èåŸ', 'æ©Ÿæ¢°'),
    '6367.T': ('ãƒ€ã‚¤ã‚­ãƒ³', 'æ©Ÿæ¢°'),
    '6471.T': ('æ—¥ç²¾å·¥', 'æ©Ÿæ¢°'),
    '6472.T': ('ï¼®ï¼´ï¼®', 'æ©Ÿæ¢°'),
    '6473.T': ('ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ', 'æ©Ÿæ¢°'),
    '6479.T': ('ãƒŸãƒãƒ™ã‚¢', 'é›»æ°—æ©Ÿå™¨'),
    '6501.T': ('æ—¥ç«‹', 'é›»æ°—æ©Ÿå™¨'),
    '6503.T': ('ä¸‰è±é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6504.T': ('å¯Œå£«é›»æ©Ÿ', 'é›»æ°—æ©Ÿå™¨'),
    '6506.T': ('å®‰å·é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6526.T': ('ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6532.T': ('ãƒ™ã‚¤ã‚«ãƒ¬ãƒ³ãƒˆ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6594.T': ('ãƒ‹ãƒ‡ãƒƒã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6645.T': ('ã‚ªãƒ ãƒ­ãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '6674.T': ('ï¼§ï¼³ãƒ¦ã‚¢ã‚µ', 'é›»æ°—æ©Ÿå™¨'),
    '6701.T': ('ï¼®ï¼¥ï¼£', 'é›»æ°—æ©Ÿå™¨'),
    '6702.T': ('å¯Œå£«é€š', 'é›»æ°—æ©Ÿå™¨'),
    '6723.T': ('ãƒ«ãƒã‚µã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6724.T': ('ã‚¨ãƒ—ã‚½ãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '6752.T': ('ãƒ‘ãƒŠï¼¨ï¼¤', 'é›»æ°—æ©Ÿå™¨'),
    '6753.T': ('ã‚·ãƒ£ãƒ¼ãƒ—', 'é›»æ°—æ©Ÿå™¨'),
    '6758.T': ('ã‚½ãƒ‹ãƒ¼ï¼§', 'é›»æ°—æ©Ÿå™¨'),
    '6762.T': ('ï¼´ï¼¤ï¼«', 'é›»æ°—æ©Ÿå™¨'),
    '6770.T': ('ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«', 'é›»æ°—æ©Ÿå™¨'),
    '6841.T': ('æ¨ªæ²³é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6857.T': ('ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ', 'é›»æ°—æ©Ÿå™¨'),
    '6861.T': ('ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6902.T': ('ãƒ‡ãƒ³ã‚½ãƒ¼', 'é›»æ°—æ©Ÿå™¨'),
    '6920.T': ('ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6952.T': ('ã‚«ã‚·ã‚ª', 'é›»æ°—æ©Ÿå™¨'),
    '6954.T': ('ãƒ•ã‚¡ãƒŠãƒƒã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6971.T': ('äº¬ã‚»ãƒ©', 'é›»æ°—æ©Ÿå™¨'),
    '6976.T': ('å¤ªé™½èª˜é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6981.T': ('æ‘ç”°è£½', 'é›»æ°—æ©Ÿå™¨'),
    '6988.T': ('æ—¥æ±é›»', 'åŒ–å­¦'),
    '7004.T': ('ã‚«ãƒŠãƒ‡ãƒ“ã‚¢', 'æ©Ÿæ¢°'),
    '7011.T': ('ä¸‰è±é‡', 'æ©Ÿæ¢°'),
    '7012.T': ('å·é‡', 'é€ èˆ¹'),
    '7013.T': ('ï¼©ï¼¨ï¼©', 'æ©Ÿæ¢°'),
    '7186.T': ('ã‚³ãƒ³ã‚³ãƒ«ãƒ‡ã‚£', 'éŠ€è¡Œ'),
    '7201.T': ('æ—¥ç”£è‡ª', 'è‡ªå‹•è»Š'),
    '7202.T': ('ã„ã™ã‚', 'è‡ªå‹•è»Š'),
    '7203.T': ('ãƒˆãƒ¨ã‚¿', 'è‡ªå‹•è»Š'),
    '7205.T': ('æ—¥é‡è‡ª', 'è‡ªå‹•è»Š'),
    '7211.T': ('ä¸‰è±è‡ª', 'è‡ªå‹•è»Š'),
    '7261.T': ('ãƒãƒ„ãƒ€', 'è‡ªå‹•è»Š'),
    '7267.T': ('ãƒ›ãƒ³ãƒ€', 'è‡ªå‹•è»Š'),
    '7269.T': ('ã‚¹ã‚ºã‚­', 'è‡ªå‹•è»Š'),
    '7270.T': ('ï¼³ï¼µï¼¢ï¼¡ï¼²ï¼µ', 'è‡ªå‹•è»Š'),
    '7272.T': ('ãƒ¤ãƒãƒç™º', 'è‡ªå‹•è»Š'),
    '7453.T': ('è‰¯å“è¨ˆç”»', 'å°å£²æ¥­'),
    '7731.T': ('ãƒ‹ã‚³ãƒ³', 'ç²¾å¯†æ©Ÿå™¨'),
    '7733.T': ('ã‚ªãƒªãƒ³ãƒ‘ã‚¹', 'ç²¾å¯†æ©Ÿå™¨'),
    '7735.T': ('ã‚¹ã‚¯ãƒªãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '7741.T': ('ï¼¨ï¼¯ï¼¹ï¼¡', 'ç²¾å¯†æ©Ÿå™¨'),
    '7751.T': ('ã‚­ãƒ¤ãƒãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '7752.T': ('ãƒªã‚³ãƒ¼', 'é›»æ°—æ©Ÿå™¨'),
    '7762.T': ('ã‚·ãƒã‚ºãƒ³', 'ç²¾å¯†æ©Ÿå™¨'),
    '7832.T': ('ãƒãƒ³ãƒŠãƒ ï¼¨ï¼¤', 'ãã®ä»–è£½é€ '),
    '7911.T': ('ï¼´ï¼¯ï¼°ï¼°ï¼¡ï¼®', 'ãã®ä»–è£½é€ '),
    '7912.T': ('å¤§æ—¥å°', 'ãã®ä»–è£½é€ '),
    '7951.T': ('ãƒ¤ãƒãƒ', 'ãã®ä»–è£½é€ '),
    '7974.T': ('ä»»å¤©å ‚', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '8001.T': ('ä¼Šè—¤å¿ ', 'å•†ç¤¾'),
    '8002.T': ('ä¸¸ç´…', 'å•†ç¤¾'),
    '8015.T': ('è±Šç”°é€šå•†', 'å•†ç¤¾'),
    '8031.T': ('ä¸‰äº•ç‰©', 'å•†ç¤¾'),
    '8035.T': ('æ±ã‚¨ãƒ¬ã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '8053.T': ('ä½å‹å•†', 'å•†ç¤¾'),
    '8058.T': ('ä¸‰è±å•†', 'å•†ç¤¾'),
    '8233.T': ('é«˜å³¶å±‹', 'å°å£²æ¥­'),
    '8252.T': ('ä¸¸äº•ï¼§', 'å°å£²æ¥­'),
    '8253.T': ('ã‚¯ãƒ¬ã‚»ã‚¾ãƒ³', 'ãã®ä»–é‡‘è'),
    '8267.T': ('ã‚¤ã‚ªãƒ³', 'å°å£²æ¥­'),
    '8304.T': ('ã‚ãŠãã‚‰éŠ€', 'éŠ€è¡Œ'),
    '8306.T': ('ä¸‰è±ï¼µï¼¦ï¼ª', 'éŠ€è¡Œ'),
    '8308.T': ('ã‚Šããªï¼¨ï¼¤', 'éŠ€è¡Œ'),
    '8309.T': ('ä¸‰äº•ä½å‹ãƒˆãƒ©', 'éŠ€è¡Œ'),
    '8316.T': ('ä¸‰äº•ä½å‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8331.T': ('åƒè‘‰éŠ€', 'éŠ€è¡Œ'),
    '8354.T': ('ãµããŠã‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8411.T': ('ã¿ãšã»ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8591.T': ('ã‚ªãƒªãƒƒã‚¯ã‚¹', 'ãã®ä»–é‡‘è'),
    '8601.T': ('å¤§å’Œ', 'è¨¼åˆ¸'),
    '8604.T': ('é‡æ‘', 'è¨¼åˆ¸'),
    '8630.T': ('ï¼³ï¼¯ï¼­ï¼°ï¼¯', 'ä¿é™º'),
    '8697.T': ('æ—¥æœ¬å–å¼•æ‰€', 'ãã®ä»–é‡‘è'),
    '8725.T': ('ï¼­ï¼³ï¼†ï¼¡ï¼¤', 'ä¿é™º'),
    '8750.T': ('ç¬¬ä¸€ç”Ÿå‘½ï¼¨ï¼¤', 'ä¿é™º'),
    '8766.T': ('æ±äº¬æµ·ä¸Š', 'ä¿é™º'),
    '8795.T': ('ï¼´ï¼†ï¼¤', 'ä¿é™º'),
    '8801.T': ('ä¸‰äº•ä¸', 'ä¸å‹•ç”£'),
    '8802.T': ('è±åœ°æ‰€', 'ä¸å‹•ç”£'),
    '8804.T': ('æ±å»ºç‰©', 'ä¸å‹•ç”£'),
    '8830.T': ('ä½å‹ä¸', 'ä¸å‹•ç”£'),
    '9001.T': ('æ±æ­¦', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9005.T': ('æ±æ€¥', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9007.T': ('å°ç”°æ€¥', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9008.T': ('äº¬ç‹', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9009.T': ('äº¬æˆ', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9020.T': ('ï¼ªï¼²æ±æ—¥æœ¬', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9021.T': ('ï¼ªï¼²è¥¿æ—¥æœ¬', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9022.T': ('ï¼ªï¼²æ±æµ·', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9064.T': ('ãƒ¤ãƒãƒˆï¼¨ï¼¤', 'é™¸é‹'),
    '9101.T': ('éƒµèˆ¹', 'æµ·é‹'),
    '9104.T': ('å•†èˆ¹ä¸‰äº•', 'æµ·é‹'),
    '9107.T': ('å·å´æ±½', 'æµ·é‹'),
    '9147.T': ('ï¼®ï¼¸ï¼¨ï¼¤', 'é™¸é‹'),
    '9201.T': ('ï¼ªï¼¡ï¼¬', 'ç©ºé‹'),
    '9202.T': ('ï¼¡ï¼®ï¼¡ï¼¨ï¼¤', 'ç©ºé‹'),
    '9432.T': ('ï¼®ï¼´ï¼´', 'é€šä¿¡'),
    '9433.T': ('ï¼«ï¼¤ï¼¤ï¼©', 'é€šä¿¡'),
    '9434.T': ('ï¼³ï¼¢', 'é€šä¿¡'),
    '9501.T': ('æ±é›»ï¼¨ï¼¤', 'é›»åŠ›'),
    '9502.T': ('ä¸­éƒ¨é›»', 'é›»åŠ›'),
    '9503.T': ('é–¢è¥¿é›»', 'é›»åŠ›'),
    '9531.T': ('æ±ã‚¬ã‚¹', 'ã‚¬ã‚¹'),
    '9532.T': ('å¤§ã‚¬ã‚¹', 'ã‚¬ã‚¹'),
    '9602.T': ('æ±å®', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9613.T': ('ï¼®ï¼´ï¼´ãƒ‡ãƒ¼ã‚¿', 'é€šä¿¡'),
    '9735.T': ('ã‚»ã‚³ãƒ ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9766.T': ('ã‚³ãƒŠãƒŸï¼§', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9843.T': ('ãƒ‹ãƒˆãƒªï¼¨ï¼¤', 'å°å£²æ¥­'),
    '9983.T': ('ãƒ•ã‚¡ã‚¹ãƒˆãƒª', 'å°å£²æ¥­'),
    '9984.T': ('ï¼³ï¼¢ï¼§', 'é€šä¿¡'),
}

# ================================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# ================================================================
@st.cache_data(ttl=3600)
def get_price(ticker, start, end):
    df = yf.download(ticker, start=start, end=end, progress=False)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.droplevel(1)
    return df

@st.cache_data(ttl=3600)
def get_benchmark(start, end):
    df = yf.download("^N225", start=start, end=end, progress=False)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.droplevel(1)
    return df

# ================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
# ================================================================
tab_analysis, tab_news, tab_market_news = st.tabs([
    "ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ",
    "ğŸ“° éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹",
    "ğŸŒ å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹",
])

# â”€â”€â”€ Tab1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_analysis:
    if st.button("â–¶ åˆ†æå®Ÿè¡Œ", type="primary"):
        end_date   = datetime.today()
        start_date = end_date - relativedelta(years=int(years))

        with st.spinner("å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥çµŒ225ï¼‰ã‚’å–å¾—ä¸­..."):
            benchmark = get_benchmark(start_date, end_date)

        if benchmark.empty:
            st.error("å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            st.stop()

        market_returns = benchmark["Close"].pct_change().dropna()

        results = []
        progress    = st.progress(0)
        status_text = st.empty()

        for i, (ticker, (name, sector)) in enumerate(ticker_name_map.items()):
            status_text.text(f"å–å¾—ä¸­: {name} ({ticker})")
            df = get_price(ticker, start_date, end_date)
            progress.progress((i + 1) / len(ticker_name_map))
            if df.empty:
                continue
            returns = df["Close"].pct_change().dropna()
            common  = returns.index.intersection(market_returns.index)
            if len(common) < 30:
                continue
            x = returns.loc[common].values.flatten()
            y = market_returns.loc[common].values.flatten()
            annual_return = x.mean() * 252
            annual_vol    = x.std() * np.sqrt(252)
            beta   = np.cov(x, y)[0][1] / np.var(y)
            sharpe = (annual_return - risk_free_rate) / annual_vol
            results.append({
                "ä¼æ¥­å": name, "æ¥­ç¨®": sector,
                "å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)": annual_return * 100,
                "å¹´é–“ãƒªã‚¹ã‚¯(%)": annual_vol * 100,
                "ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª": sharpe, "ãƒ™ãƒ¼ã‚¿": beta,
            })

        progress.empty()
        status_text.empty()

        df_results = pd.DataFrame(results)
        if df_results.empty:
            st.error("ãƒ‡ãƒ¼ã‚¿ãªã—")
            st.stop()

        df_results = df_results.sort_values("ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª", ascending=False)

        st.subheader("ğŸ“‹ åˆ†æçµæœä¸€è¦§")
        st.dataframe(
            df_results.style.format({
                "å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.2f}",
                "å¹´é–“ãƒªã‚¹ã‚¯(%)": "{:.2f}",
                "ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª": "{:.2f}",
                "ãƒ™ãƒ¼ã‚¿": "{:.2f}",
            }),
            use_container_width=True,
        )

        top_n_int   = int(top_n)
        top_stocks  = df_results.head(top_n_int)

        fig1, ax1 = plt.subplots(figsize=(14, 6))
        ax1.bar(top_stocks["ä¼æ¥­å"], top_stocks["ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª"], color="green")
        ax1.set_title(f"ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª ä¸Šä½{top_n_int}ç¤¾")
        ax1.set_ylabel("ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª")
        ax1.tick_params(axis="x", rotation=45)
        plt.tight_layout()
        st.pyplot(fig1)
        plt.close(fig1)

        fig2, ax2 = plt.subplots(figsize=(14, 6))
        ax2.bar(top_stocks["ä¼æ¥­å"], top_stocks["å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)"], color="steelblue")
        ax2.set_title(f"å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%) ä¸Šä½{top_n_int}ç¤¾")
        ax2.set_ylabel("å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)")
        ax2.tick_params(axis="x", rotation=45)
        plt.tight_layout()
        st.pyplot(fig2)
        plt.close(fig2)

        # AI ã‚³ãƒ¡ãƒ³ãƒˆ
        summary = top_stocks.head(5).to_string()
        prompt = f"""
ä»¥ä¸‹ã¯æ—¥æœ¬æ ªã®ãƒªã‚¹ã‚¯ãƒ»ãƒªã‚¿ãƒ¼ãƒ³åˆ†æçµæœã§ã™ã€‚
æŠ•è³‡å®¶å‘ã‘ã«ç°¡æ½”ã«300æ–‡å­—ä»¥å†…ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

{summary}
"""
        try:
            comment, ai_name = generate_ai_comment(prompt)
            st.subheader(f"ğŸ¤– AIã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ{ai_name}ï¼‰")
            st.write(comment)
        except Exception as e:
            st.warning(f"AI APIã‚¨ãƒ©ãƒ¼: {e}")

# â”€â”€â”€ Tab2: éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆã‚½ãƒ¼ã‚¹åˆ¥ç‹¬ç«‹è¡¨ç¤ºï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_news:
    st.subheader("ğŸ“° éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹")

    # â”€â”€ session_state åˆæœŸåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨AIè¦ç´„çµæœã‚’ãƒšãƒ¼ã‚¸å†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã‚‚ä¿æŒã™ã‚‹
    if "news_by_src" not in st.session_state:
        st.session_state.news_by_src = {}
    if "news_ticker" not in st.session_state:
        st.session_state.news_ticker = ""
    if "tdnet_summaries" not in st.session_state:
        st.session_state.tdnet_summaries = {}   # key: "{ticker}_{idx}" -> summary str
    if "sentiment_result" not in st.session_state:
        st.session_state.sentiment_result = {}  # key: ticker -> summary str

    # éŠ˜æŸ„é¸æŠ
    ticker_options = {f"{name}ï¼ˆ{t}ï¼‰": t for t, (name, _) in ticker_name_map.items()}
    default_idx = list(ticker_options.keys()).index("ãƒˆãƒ¨ã‚¿ï¼ˆ7203.Tï¼‰") if "ãƒˆãƒ¨ã‚¿ï¼ˆ7203.Tï¼‰" in ticker_options else 0
    selected_label  = st.selectbox("éŠ˜æŸ„ã‚’é¸æŠ", list(ticker_options.keys()), index=default_idx)
    selected_ticker = ticker_options[selected_label]
    selected_name   = ticker_name_map[selected_ticker][0]
    selected_code   = selected_ticker.replace(".T", "")

    # éŠ˜æŸ„ãŒå¤‰ã‚ã£ãŸã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆ
    if st.session_state.news_ticker != selected_ticker:
        st.session_state.news_by_src = {}
        st.session_state.tdnet_summaries = {}
        st.session_state.sentiment_result = {}
        st.session_state.news_ticker = selected_ticker

    col_btn1, col_btn2 = st.columns([1, 4])
    with col_btn1:
        run_news = st.button("â–¶ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—", type="primary")
    with col_btn2:
        run_ai = st.checkbox("ğŸ¤– AIã«ã‚ˆã‚‹ç·åˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ", value=True)

    # â”€â”€ ã‚½ãƒ¼ã‚¹è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SOURCE_CFG = {
        "Yahoo!Finance JP":  {"icon": "ğŸŸ¦", "label": "Yahoo!ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹",  "desc": "éŠ˜æŸ„RSS"},
        "æ ªæ¢(Kabutan)":     {"icon": "ğŸŸ©", "label": "æ ªæ¢",               "desc": "éŠ˜æŸ„å°‚ç”¨ãƒšãƒ¼ã‚¸"},
        "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰": {"icon": "ğŸ”´", "label": "TDnet é©æ™‚é–‹ç¤º",     "desc": "éå»3ãƒ¶æœˆ"},
        "æ—¥çµŒæ–°è":          {"icon": "â¬›", "label": "æ—¥çµŒæ–°è",           "desc": "éŠ˜æŸ„è¨€åŠã®ã¿"},
        "CNBC":              {"icon": "ğŸŸª", "label": "CNBC",               "desc": "è‹±èªãƒ»éŠ˜æŸ„è¨€åŠ"},
        "Reuters JP":        {"icon": "ğŸŸ«", "label": "Reuters",            "desc": "éŠ˜æŸ„è¨€åŠã®ã¿"},
    }

    # â”€â”€ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿å®Ÿè¡Œã€çµæœã¯session_stateã¸ï¼‰â”€
    if run_news:
        with st.spinner(f"{selected_name}ï¼ˆ{selected_ticker}ï¼‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ä¸¦åˆ—å–å¾—ä¸­..."):
            st.session_state.news_by_src = fetch_news_by_source(
                selected_ticker, selected_name, news_max_per_source
            )
        st.session_state.tdnet_summaries = {}   # éŠ˜æŸ„å†å–å¾—ã—ãŸã‚‰è¦ç´„ãƒªã‚»ãƒƒãƒˆ
        st.session_state.sentiment_result = {}

    # â”€â”€ å–å¾—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å¸¸ã«è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    news_by_src = st.session_state.news_by_src
    if not news_by_src:
        st.info("ã€Œâ–¶ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
    else:
        total = sum(len(v) for v in news_by_src.values())
        st.caption(f"å–å¾—å®Œäº† â€” åˆè¨ˆ {total} ä»¶")

        # â”€â”€ ã‚µãƒãƒªãƒ¼ãƒãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        cols_hdr = st.columns(len(SOURCE_CFG))
        for i, (src_key, cfg) in enumerate(SOURCE_CFG.items()):
            cnt = len(news_by_src.get(src_key, []))
            cols_hdr[i].metric(
                f"{cfg['icon']} {cfg['label']}",
                f"{cnt} ä»¶",
                help=cfg["desc"],
            )

        st.divider()

        # â”€â”€ ã‚½ãƒ¼ã‚¹ã”ã¨ã«ç‹¬ç«‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for src_key, cfg in SOURCE_CFG.items():
            items = news_by_src.get(src_key, [])
            icon  = cfg["icon"]
            label = cfg["label"]

            with st.expander(
                f"{icon} **{label}** â€” {len(items)} ä»¶  `{cfg['desc']}`",
                expanded=(len(items) > 0),
            ):
                if not items:
                    st.caption("è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    if src_key == "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰":
                        st.caption("â€» é©æ™‚é–‹ç¤ºã¯æ±ºç®—æœŸï¼ˆ3ãƒ»6ãƒ»9ãƒ»12æœˆï¼‰å‰å¾Œã«é›†ä¸­ã—ã¾ã™")
                    elif src_key in ("æ—¥çµŒæ–°è", "CNBC", "Reuters JP"):
                        st.caption(f"â€» {selected_name} ã«è¨€åŠã™ã‚‹è¨˜äº‹ãŒç›´è¿‘ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    continue

                for idx_item, item in enumerate(items):
                    title       = item["title"]
                    link        = item.get("link", "")
                    date        = item.get("date", "")
                    badge_emoji = item.get("badge_emoji", "")
                    badge_text  = item.get("badge", "")

                    col_t, col_d = st.columns([5, 1])
                    with col_t:
                        if src_key == "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰":
                            # ã‚¿ã‚¤ãƒˆãƒ«ï¼‹PDFãƒªãƒ³ã‚¯
                            if link:
                                st.markdown(f"ğŸ”´ [{title} ğŸ“„]({link})")
                            else:
                                st.markdown(f"ğŸ”´ {title} ğŸ“„")

                            # AIè¦ç´„ãƒœã‚¿ãƒ³
                            summary_key = f"{selected_code}_{idx_item}"
                            btn_key     = f"btn_tdnet_{summary_key}"

                            if st.button("ğŸ¤– AIã§è¦ç´„", key=btn_key):
                                with st.spinner("PDFå†…å®¹ã‚’å–å¾—ãƒ»è¦ç´„ä¸­..."):
                                    result = ai_summarize_tdnet_pdf(link, title)
                                # session_state ã«ä¿å­˜ â†’ ãƒœã‚¿ãƒ³å†æŠ¼ã—ã§ã‚‚æ¶ˆãˆãªã„
                                st.session_state.tdnet_summaries[summary_key] = result

                            # è¦ç´„çµæœã‚’è¡¨ç¤ºï¼ˆsession_stateã‹ã‚‰èª­ã‚€ï¼‰
                            if summary_key in st.session_state.tdnet_summaries:
                                st.info(st.session_state.tdnet_summaries[summary_key])

                        elif src_key == "æ ªæ¢(Kabutan)":
                            prefix = f"{badge_emoji}{badge_text} " if badge_text else ""
                            if link:
                                st.markdown(f"{prefix}[{title}]({link})")
                            else:
                                st.markdown(f"{prefix}{title}")

                            # æ ªæ¢ AIè¦ç´„ãƒœã‚¿ãƒ³
                            summary_key = f"kabutan_{selected_code}_{idx_item}"
                            btn_key     = f"btn_kabutan_{summary_key}"
                            if st.button("ğŸ¤– AIã§è¦ç´„", key=btn_key):
                                with st.spinner("è¨˜äº‹å†…å®¹ã‚’å–å¾—ãƒ»è¦ç´„ä¸­..."):
                                    result = ai_summarize_tdnet_pdf(link, title)
                                st.session_state.tdnet_summaries[summary_key] = result
                            if summary_key in st.session_state.tdnet_summaries:
                                st.info(st.session_state.tdnet_summaries[summary_key])

                        else:
                            prefix = f"{badge_emoji}{badge_text} " if badge_text else ""
                            if link:
                                st.markdown(f"{prefix}[{title}]({link})")
                            else:
                                st.markdown(f"{prefix}{title}")

                    with col_d:
                        if date:
                            date_short = date[:10] if len(date) >= 10 else date
                            st.caption(date_short)

                    if src_key != "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰":
                        st.markdown("---")

        # â”€â”€ AI ç·åˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if run_ai:
            st.divider()
            st.subheader(f"ğŸ¤– {selected_name} AI ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ")

            # æ—¢ã«session_stateã«çµæœãŒã‚ã‚Œã°ãã®ã¾ã¾è¡¨ç¤º
            if selected_ticker in st.session_state.sentiment_result:
                st.info(st.session_state.sentiment_result[selected_ticker])
            elif total > 0:
                all_headlines = [
                    f"[{src}] {it['title']}"
                    for src, its in news_by_src.items()
                    for it in its
                ]
                with st.spinner("AIåˆ†æä¸­..."):
                    ai_result = ai_news_summary(all_headlines, selected_name, selected_ticker)
                st.session_state.sentiment_result[selected_ticker] = ai_result
                st.info(ai_result)


# â”€â”€â”€ Tab3: å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_market_news:
    st.subheader("ğŸŒ å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥çµŒãƒ»Reutersï¼‰")

    if st.button("â–¶ å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—", type="primary"):
        import concurrent.futures

        with st.spinner("å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as ex:
                f_nikkei  = ex.submit(fetch_nikkei_market_rss, 10)
                f_reuters = ex.submit(fetch_reuters_jp_rss, 10)
                nikkei_news  = f_nikkei.result()
                reuters_news = f_reuters.result()

        col_n, col_r = st.columns(2)

        with col_n:
            st.markdown("### â¬› æ—¥çµŒæ–°è ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
            if nikkei_news:
                for item in nikkei_news:
                    st.markdown(f"- [{item['title']}]({item['link']})")
                    if item.get("date"):
                        st.caption(f"  ğŸ“… {item['date']}")
            else:
                st.info("å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆæ—¥çµŒæ–°èRSSã¯ä¼šå“¡åˆ¶ã®å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")

        with col_r:
            st.markdown("### ğŸŸ« Reuters Japan ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹")
            if reuters_news:
                for item in reuters_news:
                    st.markdown(f"- [{item['title']}]({item['link']})")
                    if item.get("date"):
                        st.caption(f"  ğŸ“… {item['date']}")
            else:
                st.info("å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        # å…¨å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’AIã§è¦ç´„
        all_market = nikkei_news + reuters_news
        if all_market and st.checkbox("ğŸ¤– å¸‚å ´å…¨ä½“ã®AIè¦ç´„ã‚’è¡¨ç¤º", value=True):
            headlines = "\n".join(f"[{n['source']}] {n['title']}" for n in all_market[:12])
            prompt = f"""
ä»¥ä¸‹ã¯æœ¬æ—¥ã®æ—¥æœ¬æ ªãƒãƒ¼ã‚±ãƒƒãƒˆé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚

{headlines}

æŠ•è³‡å®¶å‘ã‘ã«ä»¥ä¸‹ã‚’æ—¥æœ¬èª300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
1. æœ¬æ—¥ã®å¸‚å ´å…¨ä½“ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼ˆå¼·æ°—/å¼±æ°—/ä¸­ç«‹ï¼‰
2. æ³¨ç›®ãƒ†ãƒ¼ãƒãƒ»ã‚»ã‚¯ã‚¿ãƒ¼
3. ä»Šå¾Œã®æ³¨æ„ç‚¹
"""
            with st.spinner("AIè¦ç´„ä¸­..."):
                try:
                    comment, ai_name = generate_ai_comment(prompt)
                    st.subheader(f"ğŸ¤– å¸‚å ´å…¨ä½“AIè¦ç´„ï¼ˆ{ai_name}ï¼‰")
                    st.info(comment)
                except Exception as e:
                    st.warning(f"AI APIã‚¨ãƒ©ãƒ¼: {e}")
