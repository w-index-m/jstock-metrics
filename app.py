import streamlit as st
import google.generativeai as genai
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from dateutil.relativedelta import relativedelta
from groq import Groq
import requests
import xml.etree.ElementTree as ET
import re
from io import StringIO

# -----------------------------
# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
# -----------------------------
import matplotlib.font_manager as fm
import os

_FONT_PATHS = [
    "font/NotoSansCJK-Regular.ttc",
    "font/NotoSansJP-ExtraBold.ttf",
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc",
    "/usr/share/fonts/truetype/fonts-japanese-gothic.ttf",
    "/usr/share/fonts/opentype/ipafont-gothic/ipag.ttf",
]

def _set_japanese_font():
    for path in _FONT_PATHS:
        if os.path.exists(path):
            fm.fontManager.addfont(path)
            prop = fm.FontProperties(fname=path)
            plt.rcParams["font.family"] = prop.get_name()
            return prop.get_name()
    return None

_set_japanese_font()
plt.rcParams["axes.unicode_minus"] = False

# -----------------------------
# å®šæ•°
# -----------------------------
GEMINI_MODEL = "gemini-2.5-pro"
GROQ_MODEL   = "llama3-70b-8192"

# -----------------------------
# ãƒšãƒ¼ã‚¸è¨­å®š
# -----------------------------
st.set_page_config(layout="wide", page_title="ğŸ“ˆ æ—¥æœ¬æ ª åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", page_icon="ğŸ“ˆ")
st.title("ğŸ“ˆ æ—¥æœ¬æ ª ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ªåˆ†æ + ãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆ")

# -----------------------------
# AIè¨­å®šï¼ˆGeminiå„ªå…ˆ / Groqãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
# -----------------------------
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
GROQ_API_KEY   = st.secrets.get("GROQ_API_KEY", "")
genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel(GEMINI_MODEL)
groq_client  = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

def generate_ai_comment(prompt: str) -> tuple[str, str]:
    """Gemini â†’ Groq ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    try:
        response = gemini_model.generate_content(prompt)
        return response.text, "Gemini"
    except Exception as e:
        err_str = str(e)
        is_quota = "429" in err_str or "quota" in err_str.lower() or "RESOURCE_EXHAUSTED" in err_str
        if not is_quota:
            raise
    if groq_client is None:
        raise RuntimeError("Geminiã‚¯ã‚©ãƒ¼ã‚¿è¶…é & GROQ_API_KEY æœªè¨­å®š")
    chat = groq_client.chat.completions.create(
        model=GROQ_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400,
    )
    return chat.choices[0].message.content, "Groq"

# ================================================================
# ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ================================================================

_NEWS_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

# â”€â”€ â‘  Yahoo!ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ Japan RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_yahoo_jp_news(ticker_code: str, max_items: int = 8) -> list[dict]:
    """
    Yahoo!ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ Japan ã®éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã‚’å–å¾—ã€‚
    ticker_code: '7203' ãªã©ï¼ˆ.T ãªã—ï¼‰
    """
    code = ticker_code.replace(".T", "")
    url = f"https://finance.yahoo.co.jp/rss/stocks/{code}"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title = item.findtext("title", "").strip()
            link  = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            desc  = item.findtext("description", "").strip()
            # HTMLã‚¿ã‚°é™¤å»
            desc = re.sub(r"<[^>]+>", "", desc)[:100]
            if title:
                items.append({"source": "Yahoo!Finance JP", "title": title,
                              "link": link, "date": pubdate, "summary": desc})
        return items
    except Exception:
        return []


# â”€â”€ â‘¡ æ ªæ¢ï¼ˆKabutanï¼‰éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_kabutan_news(ticker_code: str, max_items: int = 8) -> list[dict]:
    """
    æ ªæ¢ã®éŠ˜æŸ„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã€‚
    URL: https://kabutan.jp/stock/news?code=XXXX

    HTMLãƒ•ã‚¡ã‚¤ãƒ«å®Ÿæ¸¬ã«ã‚ˆã‚‹ç¢ºå®šæ§‹é€ :
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    <table class="s_news_list mgbt0">
      <tbody>
        <tr>
          <td class="news_time">
            <time datetime="2026-02-19T17:00:03+09:00">26/02/19&nbsp;17:00</time>
          </td>
          <td>
            <div class="newslist_ctg newsctg5_b">ç‰¹é›†</div>
          </td>
          <td>
            <a href="https://kabutan.jp/stock/news?code=5803&b=n202602191135">
              ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ—¥å ±ã€æœ€ä¸Šä½ã‚’ç¶™ç¶šï¼‹ç›®æ¨™æ ªä¾¡ã‚’å¢—é¡ã€‘(2æœˆ19æ—¥)
            </a>
          </td>
        </tr>
        ...
        <!-- é–‹ç¤ºï¼ˆPDFï¼‰ã®å ´åˆ -->
        <tr>
          <td class="news_time"><time ...>26/02/09&nbsp;14:00</time></td>
          <td><div class="newslist_ctg newsctg_kaiji_b">é–‹ç¤º</div></td>
          <td class="td_kaiji">
            <a href="https://kabutan.jp/disclosures/pdf/20260209/140120260206550334/" target="pdf">
              2026å¹´ï¼“æœˆæœŸé€šæœŸé€£çµæ¥­ç¸¾äºˆæƒ³...
            </a>
          </td>
        </tr>
      </tbody>
    </table>
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€» ã“ã®ãƒšãƒ¼ã‚¸è‡ªä½“ãŒ code=XXXX ã®éŠ˜æŸ„å°‚ç”¨ãƒšãƒ¼ã‚¸ãªã®ã§
      å–å¾—è¨˜äº‹ã¯ã™ã¹ã¦éŠ˜æŸ„å›ºæœ‰æƒ…å ±ã€‚
    â€» ãƒªãƒ³ã‚¯URLã« &b=n... (ãƒ‹ãƒ¥ãƒ¼ã‚¹) ã¾ãŸã¯ /disclosures/pdf/... (é–‹ç¤ºPDF) ã®2ç¨®é¡ã‚ã‚Šã€‚
    â€» ãƒ—ãƒ¬ãƒŸã‚¢ãƒ è¨˜äº‹ã¯ <img class="vat pdr4"> ãŒæŒ¿å…¥ã•ã‚Œã‚‹ã€‚
    """
    code = ticker_code.replace(".T", "")
    url = f"https://kabutan.jp/stock/news?code={code}"
    headers = {
        **_NEWS_HEADERS,
        "Accept": "text/html,application/xhtml+xml",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": "https://kabutan.jp/",
    }
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200:
            return []
        html = r.text

        # â”€â”€ s_news_list ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã‚’å–å¾—
        table_match = re.search(
            r'class="s_news_list[^"]*"[^>]*>(.*?)</table>',
            html, re.DOTALL
        )
        if not table_match:
            return []
        table_html = table_match.group(1)

        # â”€â”€ tr è¡Œã”ã¨ã«ãƒ‘ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        rows = re.findall(r'<tr[^>]*>(.*?)</tr>', table_html, re.DOTALL)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šãƒãƒƒãƒ—
        ctg_class_map = {
            "newsctg2_b":    "ææ–™",
            "newsctg3_kk_b": "æ±ºç®—",
            "newsctg4_b":    "ãƒ†ã‚¯",
            "newsctg5_b":    "ç‰¹é›†",
            "newsctg_kaiji_b": "é–‹ç¤º",
        }
        badge_emoji = {
            "ææ–™": "ğŸŸ¢", "æ±ºç®—": "ğŸ”µ", "ãƒ†ã‚¯": "âšª",
            "ç‰¹é›†": "ğŸŸ ", "é–‹ç¤º": "ğŸ”´",
        }

        items = []
        for row in rows:
            # â‘  æ—¥æ™‚: <time datetime="2026-02-19T17:00:03+09:00">
            time_match = re.search(r'<time[^>]+datetime="([^"]+)"', row)
            if not time_match:
                continue
            # datetimeå±æ€§ã‹ã‚‰èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
            dt_raw = time_match.group(1)  # "2026-02-19T17:00:03+09:00"
            dt_disp = re.search(r'(\d{4}-\d{2}-\d{2})T(\d{2}:\d{2})', dt_raw)
            date_str = f"{dt_disp.group(1)} {dt_disp.group(2)}" if dt_disp else dt_raw[:16]

            # â‘¡ ã‚«ãƒ†ã‚´ãƒªãƒãƒƒã‚¸: class="newslist_ctg newsctgX_b"
            badge = ""
            for cls, label in ctg_class_map.items():
                if cls in row:
                    badge = label
                    break

            # â‘¢ ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«: 2ãƒ‘ã‚¿ãƒ¼ãƒ³
            #    a) ãƒ‹ãƒ¥ãƒ¼ã‚¹: href="https://kabutan.jp/stock/news?code=XXXX&b=nXXX"
            #    b) é–‹ç¤ºPDF:  href="https://kabutan.jp/disclosures/pdf/..."
            link_match = re.search(
                r'<a\s+href="(https://kabutan\.jp/(?:stock/news\?[^"]+|disclosures/pdf/[^"]+))"'
                r'[^>]*>\s*(.*?)\s*</a>',
                row, re.DOTALL
            )
            if not link_match:
                continue

            link = link_match.group(1).replace("&amp;", "&")
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰HTMLã‚¿ã‚°ï¼ˆimgãªã©ï¼‰ã‚’é™¤å»
            title = re.sub(r'<[^>]+>', '', link_match.group(2)).strip()

            if len(title) < 3:
                continue

            # â‘£ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ è¨˜äº‹ã®æ¤œå‡ºï¼ˆãƒ­ãƒƒã‚¯ç”»åƒãŒæŒ¿å…¥ã•ã‚Œã‚‹ï¼‰
            is_premium = "ğŸ”’ " if "premium" in row.lower() or "pdr4" in row else ""

            emoji = badge_emoji.get(badge, "ğŸ“°")

            items.append({
                "source": "æ ªæ¢(Kabutan)",
                "title": f"{is_premium}{title}",
                "badge": badge,
                "badge_emoji": emoji,
                "link": link,
                "date": date_str,
                "summary": "",
                "ticker_specific": True,
            })

            if len(items) >= max_items:
                break

        return items

    except Exception:
        return []


# â”€â”€ â‘¢ ã¿ã‚“ã‹ã¶ éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆRSSä½¿ç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_minkabu_news(ticker_code: str, max_items: int = 6) -> list[dict]:
    """
    ã¿ã‚“ã‹ã¶ã®éŠ˜æŸ„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‚
    ã¿ã‚“ã‹ã¶ã¯ /stock/{code}/news ãƒšãƒ¼ã‚¸ã§éŠ˜æŸ„å›ºæœ‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æä¾›ã€‚
    """
    code = ticker_code.replace(".T", "")
    url = f"https://minkabu.jp/stock/{code}/news"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=12)
        if r.status_code != 200:
            return []
        html = r.text

        # ã¿ã‚“ã‹ã¶ã®éŠ˜æŸ„ãƒ‹ãƒ¥ãƒ¼ã‚¹æ§‹é€ :
        # <li class="news_list_item"> ... <a href="/news/...">ã‚¿ã‚¤ãƒˆãƒ«</a>
        # ã¾ãŸã¯ <a href="/stock/XXXX/news/XXXXX">
        pattern = re.compile(
            r'<a\s+href="((?:/stock/' + code + r'/news/|/news/)[^"]+)"[^>]*>\s*([^<]{4,120})\s*</a>',
        )
        matches = pattern.findall(html)

        # æ—¥ä»˜æŠ½å‡º
        dates = re.findall(r'(\d{4}/\d{2}/\d{2}|\d{2}/\d{2}\s+\d{2}:\d{2})', html)

        items = []
        seen = set()
        for i, (path, title) in enumerate(matches[:max_items * 2]):
            title = title.strip()
            if len(title) < 4 or title in seen:
                continue
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç³»ã‚’é™¤å¤–
            if any(kw in title for kw in ["ãƒ­ã‚°ã‚¤ãƒ³", "ä¼šå“¡ç™»éŒ²", "ã¿ã‚“ã‹ã¶", "è©³ã—ãè¦‹ã‚‹"]):
                continue
            seen.add(title)
            link = f"https://minkabu.jp{path}" if path.startswith("/") else path
            date = dates[i] if i < len(dates) else ""
            items.append({
                "source": "ã¿ã‚“ã‹ã¶",
                "title": title,
                "link": link,
                "date": date,
                "summary": "",
                "ticker_specific": True,
            })
            if len(items) >= max_items:
                break
        return items
    except Exception:
        return []


# â”€â”€ â‘£ TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰éŠ˜æŸ„åˆ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=900)
def fetch_tdnet_news(ticker_code: str, max_items: int = 6) -> list[dict]:
    """
    EDINET/JPXãŒæä¾›ã™ã‚‹TDneté–‹ç¤ºæƒ…å ±ã€‚
    JPXã®é©æ™‚é–‹ç¤ºæƒ…å ±é–²è¦§ã‚µãƒ¼ãƒ“ã‚¹ã§éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŒ‡å®šæ¤œç´¢ã‚’ä½¿ç”¨ã€‚
    URL: https://www.release.tdnet.info/inbs/I_list_001_{date}.html
    éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‚
    """
    code = ticker_code.replace(".T", "")
    today = datetime.today().strftime("%Y%m%d")

    # å½“æ—¥ã®é–‹ç¤ºä¸€è¦§ã‹ã‚‰éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã§çµã‚‹
    url = f"https://www.release.tdnet.info/inbs/I_list_001_{today}.html"
    try:
        r = requests.get(url, headers={
            **_NEWS_HEADERS,
            "Host": "www.release.tdnet.info",
            "Referer": "https://www.release.tdnet.info/",
        }, timeout=15)
        if r.status_code != 200:
            # å‰æ—¥ã‚‚è©¦ã™
            import datetime as dt
            yesterday = (dt.date.today() - dt.timedelta(days=1)).strftime("%Y%m%d")
            r = requests.get(
                f"https://www.release.tdnet.info/inbs/I_list_001_{yesterday}.html",
                headers=_NEWS_HEADERS, timeout=15
            )
            if r.status_code != 200:
                return []

        html = r.text

        # TDnetã®è¡¨æ§‹é€ : <td>è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰</td><td>ä¼šç¤¾å</td><td>é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«</td>
        # ã‚³ãƒ¼ãƒ‰ã§ãƒãƒƒãƒã™ã‚‹è¡Œã‚’æ¢ã™
        # è¡Œå˜ä½ã§ãƒ‘ãƒ¼ã‚¹: <tr>...</tr> ã®ä¸­ã« code ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æŠ½å‡º
        rows = re.findall(r'<tr[^>]*>(.*?)</tr>', html, re.DOTALL)
        items = []
        for row in rows:
            # è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã‚’ç¢ºèª
            cells = re.findall(r'<td[^>]*>(.*?)</td>', row, re.DOTALL)
            clean_cells = [re.sub(r"<[^>]+>", "", c).strip() for c in cells]
            if not any(code in c for c in clean_cells):
                continue
            # PDFãƒªãƒ³ã‚¯å–å¾—
            pdf_match = re.search(r'href="([^"]+\.pdf)"', row)
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã®æ¬¡ã®ã‚»ãƒ«ã‚ãŸã‚Šï¼‰
            title = ""
            for j, c in enumerate(clean_cells):
                if code in c and j + 2 < len(clean_cells):
                    title = clean_cells[j + 2]  # ã‚³ãƒ¼ãƒ‰â†’ä¼šç¤¾åâ†’ã‚¿ã‚¤ãƒˆãƒ«ã®é †
                    break
            if not title:
                # ã‚¯ãƒ©ã‚¹å kjTitle ã®ã‚»ãƒ«ã‚’æ¢ã™
                title_match = re.search(r'class="[^"]*kjTitle[^"]*"[^>]*>(.*?)</td>', row, re.DOTALL)
                if title_match:
                    title = re.sub(r"<[^>]+>", "", title_match.group(1)).strip()
            # æ—¥æ™‚
            time_match = re.search(r'(\d{2}:\d{2})', row)
            time_str = time_match.group(1) if time_match else ""

            if not title or len(title) < 2:
                continue
            link = ""
            if pdf_match:
                pdf_path = pdf_match.group(1)
                link = f"https://www.release.tdnet.info{pdf_path}" if pdf_path.startswith("/") else pdf_path

            items.append({
                "source": "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰",
                "title": title,
                "link": link,
                "date": f"æœ¬æ—¥ {time_str}" if time_str else "æœ¬æ—¥",
                "summary": "ğŸ“„ é©æ™‚é–‹ç¤ºPDF",
                "ticker_specific": True,
            })
            if len(items) >= max_items:
                break
        return items
    except Exception:
        return []


# â”€â”€ â‘¤ æ—¥çµŒæ–°è ãƒãƒ¼ã‚±ãƒƒãƒˆ RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_nikkei_market_rss(max_items: int = 8) -> list[dict]:
    """æ—¥çµŒæ–°èãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹ RSSï¼ˆå…¨ä½“å¸‚æ³ï¼‰"""
    url = "https://www.nikkei.com/rss/market.xml"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title   = item.findtext("title", "").strip()
            link    = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            if title:
                items.append({"source": "æ—¥çµŒæ–°è", "title": title,
                              "link": link, "date": pubdate, "summary": ""})
        return items
    except Exception:
        return []


# â”€â”€ â‘¥ Reuters Japan RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=600)
def fetch_reuters_jp_rss(max_items: int = 8) -> list[dict]:
    """Reutersæ—¥æœ¬èªãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹"""
    url = "https://feeds.reuters.com/reuters/JPBusinessNews"
    try:
        r = requests.get(url, headers=_NEWS_HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        root = ET.fromstring(r.content)
        items = []
        for item in root.findall(".//item")[:max_items]:
            title   = item.findtext("title", "").strip()
            link    = item.findtext("link", "").strip()
            pubdate = item.findtext("pubDate", "").strip()
            if title:
                items.append({"source": "Reuters JP", "title": title,
                              "link": link, "date": pubdate, "summary": ""})
        return items
    except Exception:
        return []


# â”€â”€ â‘¦ çµ±åˆãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆéŠ˜æŸ„åˆ¥ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_all_news(
    ticker_code: str,
    company_name: str,
    max_per_source: int = 5,
) -> list[dict]:
    """
    å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‚’ä¸¦åˆ—å–å¾—ã€‚
    - éŠ˜æŸ„å›ºæœ‰ã‚½ãƒ¼ã‚¹ï¼ˆYahoo!JP / æ ªæ¢ / ã¿ã‚“ã‹ã¶ / TDnetï¼‰: ãã®ã¾ã¾ä½¿ç”¨
    - å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥çµŒ / Reutersï¼‰: éŠ˜æŸ„åãƒ»ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¨˜äº‹ã®ã¿æ®‹ã™
    è¿”ã‚Šå€¤: [{source, title, link, date, summary, ticker_specific}, ...]
    """
    import concurrent.futures
    code = ticker_code.replace(".T", "")

    tasks = {
        "yahoo_jp": lambda: fetch_yahoo_jp_news(code, max_per_source),
        "kabutan":  lambda: fetch_kabutan_news(code, max_per_source),
        "minkabu":  lambda: fetch_minkabu_news(code, max_per_source),
        "tdnet":    lambda: fetch_tdnet_news(code, max_per_source),
        "nikkei":   lambda: fetch_nikkei_market_rss(max_per_source * 3),  # å¤šã‚å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
        "reuters":  lambda: fetch_reuters_jp_rss(max_per_source * 3),
    }

    # éŠ˜æŸ„ãƒãƒƒãƒç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ»ä¼šç¤¾åã®ä¸€éƒ¨ï¼‰
    # ä¼šç¤¾åã®æ‹¬å¼§ãƒ»ç‰¹æ®Šæ–‡å­—ã‚’é™¤ã„ãŸã‚·ãƒ³ãƒ—ãƒ«ãªå½¢ã«ã™ã‚‹
    company_short = re.sub(r"[ã€€ï¼¨ï¼¤ï¼ˆï¼‰()ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹]", "", company_name)[:4]
    match_keywords = {code, company_name, company_short}
    match_keywords = {k for k in match_keywords if len(k) >= 2}

    all_items = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as ex:
        futures = {ex.submit(fn): key for key, fn in tasks.items()}
        for future in concurrent.futures.as_completed(futures):
            key = futures[future]
            try:
                results = future.result()
                # å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥çµŒãƒ»Reutersï¼‰ã¯éŠ˜æŸ„é–¢é€£è¨˜äº‹ã®ã¿æ®‹ã™
                if key in ("nikkei", "reuters"):
                    results = [
                        item for item in results
                        if any(kw in item.get("title", "") for kw in match_keywords)
                    ]
                    for item in results:
                        item["ticker_specific"] = False
                all_items.extend(results)
            except Exception:
                pass

    # é‡è¤‡é™¤å»
    seen, unique = set(), []
    for item in all_items:
        key = item["title"][:30]
        if key not in seen:
            seen.add(key)
            unique.append(item)

    # ã‚½ãƒ¼ãƒˆ: éŠ˜æŸ„å›ºæœ‰ã‚’å…ˆé ­ã€æ—¥æ™‚ã®æ–°ã—ã„é †
    unique.sort(key=lambda x: (not x.get("ticker_specific", True), x.get("date", "")), reverse=False)
    unique.sort(key=lambda x: not x.get("ticker_specific", True))

    return unique


# â”€â”€ â‘§ AI ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ai_news_summary(news_items: list[dict], company_name: str, ticker: str) -> str:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’AIã§æ—¥æœ¬èªè¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
    if not news_items:
        return "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    headlines = "\n".join(
        f"[{it['source']}] {it['title']}" for it in news_items[:15]
    )
    prompt = f"""
ä»¥ä¸‹ã¯æ—¥æœ¬æ ªã€Œ{company_name}ï¼ˆ{ticker}ï¼‰ã€ã«é–¢ã™ã‚‹æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»é©æ™‚é–‹ç¤ºã®è¦‹å‡ºã—ã§ã™ã€‚

{headlines}

æŠ•è³‡å®¶å‘ã‘ã«ä»¥ä¸‹ã‚’æ—¥æœ¬èª300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
1. ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ¤å®š: ã€å¼·æ°— / å¼±æ°— / ä¸­ç«‹ã€‘
2. æ³¨ç›®ã‚¤ãƒ™ãƒ³ãƒˆã®è¦ç‚¹
3. æ ªä¾¡ã¸ã®å½±éŸ¿ã®å¯èƒ½æ€§
"""
    try:
        comment, ai_name = generate_ai_comment(prompt)
        return f"{comment}\n\n_AI: {ai_name}_"
    except Exception as e:
        return f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}"


# ================================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼
# ================================================================
with st.sidebar:
    st.header("âš™ï¸ åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    years          = st.number_input("ğŸ“… éå»ä½•å¹´ã§åˆ†æï¼Ÿ", 1, 10, 3)
    risk_free_rate = st.number_input("ğŸ“‰ ç„¡ãƒªã‚¹ã‚¯é‡‘åˆ©ï¼ˆ%ï¼‰", 0.0, 10.0, 1.0, step=0.1) / 100
    top_n          = st.number_input("ğŸ“Š ä¸Šä½ä½•ç¤¾ã‚’è¡¨ç¤ºï¼Ÿ", 5, 50, 20, step=5)
    st.divider()

    st.header("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨­å®š")
    news_max_per_source = st.slider("å„ã‚½ãƒ¼ã‚¹ã®æœ€å¤§å–å¾—ä»¶æ•°", 3, 10, 5)
    show_news_sources = st.multiselect(
        "è¡¨ç¤ºã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹",
        ["Yahoo!Finance JP", "æ ªæ¢(Kabutan)", "ã¿ã‚“ã‹ã¶", "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰", "æ—¥çµŒæ–°è", "Reuters JP"],
        default=["Yahoo!Finance JP", "æ ªæ¢(Kabutan)", "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰", "æ—¥çµŒæ–°è", "Reuters JP"],
    )
    st.divider()
    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: Yahoo Finance, TDnet, æ ªæ¢, ã¿ã‚“ã‹ã¶, æ—¥çµŒ, Reuters")

# ================================================================
# éŠ˜æŸ„ãƒã‚¹ã‚¿
# ================================================================
ticker_name_map = {
    '1332.T': ('ãƒ‹ãƒƒã‚¹ã‚¤', 'æ°´ç”£'),
    '1605.T': ('ï¼©ï¼®ï¼°ï¼¥ï¼¸', 'é‰±æ¥­'),
    '1721.T': ('ã‚³ãƒ ã‚·ã‚¹ï¼¨ï¼¤', 'å»ºè¨­'),
    '1801.T': ('å¤§æˆå»º', 'å»ºè¨­'),
    '1802.T': ('å¤§æ—çµ„', 'å»ºè¨­'),
    '1803.T': ('æ¸…æ°´å»º', 'å»ºè¨­'),
    '1808.T': ('é•·è°·å·¥', 'å»ºè¨­'),
    '1812.T': ('é¹¿å³¶', 'å»ºè¨­'),
    '1925.T': ('ãƒã‚¦ã‚¹', 'å»ºè¨­'),
    '1928.T': ('ç©ãƒã‚¦ã‚¹', 'å»ºè¨­'),
    '1963.T': ('æ—¥æ®ï¼¨ï¼¤', 'å»ºè¨­'),
    '2002.T': ('æ—¥æ¸…ç²‰ï¼§', 'é£Ÿå“'),
    '2269.T': ('æ˜æ²»ï¼¨ï¼¤', 'é£Ÿå“'),
    '2282.T': ('æ—¥ãƒãƒ ', 'é£Ÿå“'),
    '2413.T': ('ã‚¨ãƒ ã‚¹ãƒªãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '2432.T': ('ãƒ‡ã‚£ãƒ¼ã‚¨ãƒŒã‚¨', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '2501.T': ('ã‚µãƒƒãƒãƒ­ï¼¨ï¼¤', 'é£Ÿå“'),
    '2502.T': ('ã‚¢ã‚µãƒ’', 'é£Ÿå“'),
    '2503.T': ('ã‚­ãƒªãƒ³ï¼¨ï¼¤', 'é£Ÿå“'),
    '2768.T': ('åŒæ—¥', 'å•†ç¤¾'),
    '2801.T': ('ã‚­ãƒƒã‚³ãƒãƒ³', 'é£Ÿå“'),
    '2802.T': ('å‘³ã®ç´ ', 'é£Ÿå“'),
    '2871.T': ('ãƒ‹ãƒãƒ¬ã‚¤', 'é£Ÿå“'),
    '2914.T': ('ï¼ªï¼´', 'é£Ÿå“'),
    '3086.T': ('ï¼ªãƒ•ãƒ­ãƒ³ãƒˆ', 'å°å£²æ¥­'),
    '3092.T': ('ï¼ºï¼¯ï¼ºï¼¯', 'å°å£²æ¥­'),
    '3099.T': ('ä¸‰è¶Šä¼Šå‹¢ä¸¹', 'å°å£²æ¥­'),
    '3289.T': ('æ±æ€¥ä¸ï¼¨ï¼¤', 'ä¸å‹•ç”£'),
    '3382.T': ('ã‚»ãƒ–ãƒ³ï¼†ã‚¢ã‚¤', 'å°å£²æ¥­'),
    '3401.T': ('å¸äºº', 'ç¹Šç¶­'),
    '3402.T': ('æ±ãƒ¬', 'ç¹Šç¶­'),
    '3405.T': ('ã‚¯ãƒ©ãƒ¬', 'åŒ–å­¦'),
    '3407.T': ('æ—­åŒ–æˆ', 'åŒ–å­¦'),
    '3436.T': ('ï¼³ï¼µï¼­ï¼£ï¼¯', 'éé‰„ãƒ»é‡‘å±'),
    '3659.T': ('ãƒã‚¯ã‚½ãƒ³', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '3861.T': ('ç‹å­ï¼¨ï¼¤', 'ãƒ‘ãƒ«ãƒ—ãƒ»ç´™'),
    '4004.T': ('ãƒ¬ã‚¾ãƒŠãƒƒã‚¯', 'åŒ–å­¦'),
    '4005.T': ('ä½å‹åŒ–', 'åŒ–å­¦'),
    '4021.T': ('æ—¥ç”£åŒ–', 'åŒ–å­¦'),
    '4042.T': ('æ±ã‚½ãƒ¼', 'åŒ–å­¦'),
    '4043.T': ('ãƒˆã‚¯ãƒ¤ãƒ', 'åŒ–å­¦'),
    '4061.T': ('ãƒ‡ãƒ³ã‚«', 'åŒ–å­¦'),
    '4063.T': ('ä¿¡è¶ŠåŒ–', 'åŒ–å­¦'),
    '4151.T': ('å”å’Œã‚­ãƒªãƒ³', 'åŒ»è–¬å“'),
    '4183.T': ('ä¸‰äº•åŒ–å­¦', 'åŒ–å­¦'),
    '4188.T': ('ä¸‰è±ã‚±ãƒŸï¼§', 'åŒ–å­¦'),
    '4208.T': ('ï¼µï¼¢ï¼¥', 'åŒ–å­¦'),
    '4307.T': ('é‡æ‘ç·ç ”', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4324.T': ('é›»é€šã‚°ãƒ«ãƒ¼ãƒ—', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4385.T': ('ãƒ¡ãƒ«ã‚«ãƒª', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4452.T': ('èŠ±ç‹', 'åŒ–å­¦'),
    '4502.T': ('æ­¦ç”°', 'åŒ»è–¬å“'),
    '4503.T': ('ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹', 'åŒ»è–¬å“'),
    '4506.T': ('ä½å‹ãƒ•ã‚¡ãƒ¼ãƒ', 'åŒ»è–¬å“'),
    '4507.T': ('å¡©é‡ç¾©', 'åŒ»è–¬å“'),
    '4519.T': ('ä¸­å¤–è–¬', 'åŒ»è–¬å“'),
    '4523.T': ('ã‚¨ãƒ¼ã‚¶ã‚¤', 'åŒ»è–¬å“'),
    '4543.T': ('ãƒ†ãƒ«ãƒ¢', 'ç²¾å¯†æ©Ÿå™¨'),
    '4568.T': ('ç¬¬ä¸€ä¸‰å…±', 'åŒ»è–¬å“'),
    '4578.T': ('å¤§å¡šï¼¨ï¼¤', 'åŒ»è–¬å“'),
    '4661.T': ('ï¼¯ï¼¬ï¼£', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4689.T': ('ãƒ©ã‚¤ãƒ³ãƒ¤ãƒ•ãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4704.T': ('ãƒˆãƒ¬ãƒ³ãƒ‰', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4751.T': ('ã‚µã‚¤ãƒãƒ¼', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4755.T': ('æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '4901.T': ('å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ', 'åŒ–å­¦'),
    '4902.T': ('ã‚³ãƒ‹ã‚«ãƒŸãƒãƒ«', 'ç²¾å¯†æ©Ÿå™¨'),
    '4911.T': ('è³‡ç”Ÿå ‚', 'åŒ–å­¦'),
    '5019.T': ('å‡ºå…‰èˆˆç”£', 'çŸ³æ²¹'),
    '5020.T': ('ï¼¥ï¼®ï¼¥ï¼¯ï¼³', 'çŸ³æ²¹'),
    '5101.T': ('æµœã‚´ãƒ ', 'ã‚´ãƒ '),
    '5108.T': ('ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³', 'ã‚´ãƒ '),
    '5201.T': ('ï¼¡ï¼§ï¼£', 'çª¯æ¥­'),
    '5214.T': ('æ—¥é›»ç¡', 'çª¯æ¥­'),
    '5233.T': ('å¤ªå¹³æ´‹ã‚»ãƒ¡', 'çª¯æ¥­'),
    '5301.T': ('æ±æµ·ã‚«ãƒ¼ãƒœãƒ³', 'çª¯æ¥­'),
    '5332.T': ('ï¼´ï¼¯ï¼´ï¼¯', 'çª¯æ¥­'),
    '5333.T': ('ã‚¬ã‚¤ã‚·', 'çª¯æ¥­'),
    '5401.T': ('æ—¥æœ¬è£½é‰„', 'é‰„é‹¼'),
    '5406.T': ('ç¥æˆ¸é‹¼', 'é‰„é‹¼'),
    '5411.T': ('ï¼ªï¼¦ï¼¥', 'é‰„é‹¼'),
    '5631.T': ('æ—¥è£½é‹¼', 'æ©Ÿæ¢°'),
    '5706.T': ('ä¸‰äº•é‡‘', 'éé‰„ãƒ»é‡‘å±'),
    '5711.T': ('ä¸‰è±ãƒ', 'éé‰„ãƒ»é‡‘å±'),
    '5713.T': ('ä½å‹é‰±', 'éé‰„ãƒ»é‡‘å±'),
    '5714.T': ('ï¼¤ï¼¯ï¼·ï¼¡', 'éé‰„ãƒ»é‡‘å±'),
    '5801.T': ('å¤æ²³é›»', 'éé‰„ãƒ»é‡‘å±'),
    '5802.T': ('ä½å‹é›»', 'éé‰„ãƒ»é‡‘å±'),
    '5803.T': ('ãƒ•ã‚¸ã‚¯ãƒ©', 'éé‰„ãƒ»é‡‘å±'),
    '5831.T': ('ã—ãšãŠã‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '6098.T': ('ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6103.T': ('ã‚ªãƒ¼ã‚¯ãƒ', 'æ©Ÿæ¢°'),
    '6113.T': ('ã‚¢ãƒãƒ€', 'æ©Ÿæ¢°'),
    '6146.T': ('ãƒ‡ã‚£ã‚¹ã‚³', 'ç²¾å¯†æ©Ÿå™¨'),
    '6178.T': ('æ—¥æœ¬éƒµæ”¿', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6273.T': ('ï¼³ï¼­ï¼£', 'æ©Ÿæ¢°'),
    '6301.T': ('ã‚³ãƒãƒ„', 'æ©Ÿæ¢°'),
    '6302.T': ('ä½å‹é‡', 'æ©Ÿæ¢°'),
    '6305.T': ('æ—¥ç«‹å»ºæ©Ÿ', 'æ©Ÿæ¢°'),
    '6326.T': ('ã‚¯ãƒœã‚¿', 'æ©Ÿæ¢°'),
    '6361.T': ('èåŸ', 'æ©Ÿæ¢°'),
    '6367.T': ('ãƒ€ã‚¤ã‚­ãƒ³', 'æ©Ÿæ¢°'),
    '6471.T': ('æ—¥ç²¾å·¥', 'æ©Ÿæ¢°'),
    '6472.T': ('ï¼®ï¼´ï¼®', 'æ©Ÿæ¢°'),
    '6473.T': ('ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ', 'æ©Ÿæ¢°'),
    '6479.T': ('ãƒŸãƒãƒ™ã‚¢', 'é›»æ°—æ©Ÿå™¨'),
    '6501.T': ('æ—¥ç«‹', 'é›»æ°—æ©Ÿå™¨'),
    '6503.T': ('ä¸‰è±é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6504.T': ('å¯Œå£«é›»æ©Ÿ', 'é›»æ°—æ©Ÿå™¨'),
    '6506.T': ('å®‰å·é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6526.T': ('ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6532.T': ('ãƒ™ã‚¤ã‚«ãƒ¬ãƒ³ãƒˆ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '6594.T': ('ãƒ‹ãƒ‡ãƒƒã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6645.T': ('ã‚ªãƒ ãƒ­ãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '6674.T': ('ï¼§ï¼³ãƒ¦ã‚¢ã‚µ', 'é›»æ°—æ©Ÿå™¨'),
    '6701.T': ('ï¼®ï¼¥ï¼£', 'é›»æ°—æ©Ÿå™¨'),
    '6702.T': ('å¯Œå£«é€š', 'é›»æ°—æ©Ÿå™¨'),
    '6723.T': ('ãƒ«ãƒã‚µã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6724.T': ('ã‚¨ãƒ—ã‚½ãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '6752.T': ('ãƒ‘ãƒŠï¼¨ï¼¤', 'é›»æ°—æ©Ÿå™¨'),
    '6753.T': ('ã‚·ãƒ£ãƒ¼ãƒ—', 'é›»æ°—æ©Ÿå™¨'),
    '6758.T': ('ã‚½ãƒ‹ãƒ¼ï¼§', 'é›»æ°—æ©Ÿå™¨'),
    '6762.T': ('ï¼´ï¼¤ï¼«', 'é›»æ°—æ©Ÿå™¨'),
    '6770.T': ('ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«', 'é›»æ°—æ©Ÿå™¨'),
    '6841.T': ('æ¨ªæ²³é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6857.T': ('ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ', 'é›»æ°—æ©Ÿå™¨'),
    '6861.T': ('ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹', 'é›»æ°—æ©Ÿå™¨'),
    '6902.T': ('ãƒ‡ãƒ³ã‚½ãƒ¼', 'é›»æ°—æ©Ÿå™¨'),
    '6920.T': ('ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6952.T': ('ã‚«ã‚·ã‚ª', 'é›»æ°—æ©Ÿå™¨'),
    '6954.T': ('ãƒ•ã‚¡ãƒŠãƒƒã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '6971.T': ('äº¬ã‚»ãƒ©', 'é›»æ°—æ©Ÿå™¨'),
    '6976.T': ('å¤ªé™½èª˜é›»', 'é›»æ°—æ©Ÿå™¨'),
    '6981.T': ('æ‘ç”°è£½', 'é›»æ°—æ©Ÿå™¨'),
    '6988.T': ('æ—¥æ±é›»', 'åŒ–å­¦'),
    '7004.T': ('ã‚«ãƒŠãƒ‡ãƒ“ã‚¢', 'æ©Ÿæ¢°'),
    '7011.T': ('ä¸‰è±é‡', 'æ©Ÿæ¢°'),
    '7012.T': ('å·é‡', 'é€ èˆ¹'),
    '7013.T': ('ï¼©ï¼¨ï¼©', 'æ©Ÿæ¢°'),
    '7186.T': ('ã‚³ãƒ³ã‚³ãƒ«ãƒ‡ã‚£', 'éŠ€è¡Œ'),
    '7201.T': ('æ—¥ç”£è‡ª', 'è‡ªå‹•è»Š'),
    '7202.T': ('ã„ã™ã‚', 'è‡ªå‹•è»Š'),
    '7203.T': ('ãƒˆãƒ¨ã‚¿', 'è‡ªå‹•è»Š'),
    '7205.T': ('æ—¥é‡è‡ª', 'è‡ªå‹•è»Š'),
    '7211.T': ('ä¸‰è±è‡ª', 'è‡ªå‹•è»Š'),
    '7261.T': ('ãƒãƒ„ãƒ€', 'è‡ªå‹•è»Š'),
    '7267.T': ('ãƒ›ãƒ³ãƒ€', 'è‡ªå‹•è»Š'),
    '7269.T': ('ã‚¹ã‚ºã‚­', 'è‡ªå‹•è»Š'),
    '7270.T': ('ï¼³ï¼µï¼¢ï¼¡ï¼²ï¼µ', 'è‡ªå‹•è»Š'),
    '7272.T': ('ãƒ¤ãƒãƒç™º', 'è‡ªå‹•è»Š'),
    '7453.T': ('è‰¯å“è¨ˆç”»', 'å°å£²æ¥­'),
    '7731.T': ('ãƒ‹ã‚³ãƒ³', 'ç²¾å¯†æ©Ÿå™¨'),
    '7733.T': ('ã‚ªãƒªãƒ³ãƒ‘ã‚¹', 'ç²¾å¯†æ©Ÿå™¨'),
    '7735.T': ('ã‚¹ã‚¯ãƒªãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '7741.T': ('ï¼¨ï¼¯ï¼¹ï¼¡', 'ç²¾å¯†æ©Ÿå™¨'),
    '7751.T': ('ã‚­ãƒ¤ãƒãƒ³', 'é›»æ°—æ©Ÿå™¨'),
    '7752.T': ('ãƒªã‚³ãƒ¼', 'é›»æ°—æ©Ÿå™¨'),
    '7762.T': ('ã‚·ãƒã‚ºãƒ³', 'ç²¾å¯†æ©Ÿå™¨'),
    '7832.T': ('ãƒãƒ³ãƒŠãƒ ï¼¨ï¼¤', 'ãã®ä»–è£½é€ '),
    '7911.T': ('ï¼´ï¼¯ï¼°ï¼°ï¼¡ï¼®', 'ãã®ä»–è£½é€ '),
    '7912.T': ('å¤§æ—¥å°', 'ãã®ä»–è£½é€ '),
    '7951.T': ('ãƒ¤ãƒãƒ', 'ãã®ä»–è£½é€ '),
    '7974.T': ('ä»»å¤©å ‚', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '8001.T': ('ä¼Šè—¤å¿ ', 'å•†ç¤¾'),
    '8002.T': ('ä¸¸ç´…', 'å•†ç¤¾'),
    '8015.T': ('è±Šç”°é€šå•†', 'å•†ç¤¾'),
    '8031.T': ('ä¸‰äº•ç‰©', 'å•†ç¤¾'),
    '8035.T': ('æ±ã‚¨ãƒ¬ã‚¯', 'é›»æ°—æ©Ÿå™¨'),
    '8053.T': ('ä½å‹å•†', 'å•†ç¤¾'),
    '8058.T': ('ä¸‰è±å•†', 'å•†ç¤¾'),
    '8233.T': ('é«˜å³¶å±‹', 'å°å£²æ¥­'),
    '8252.T': ('ä¸¸äº•ï¼§', 'å°å£²æ¥­'),
    '8253.T': ('ã‚¯ãƒ¬ã‚»ã‚¾ãƒ³', 'ãã®ä»–é‡‘è'),
    '8267.T': ('ã‚¤ã‚ªãƒ³', 'å°å£²æ¥­'),
    '8304.T': ('ã‚ãŠãã‚‰éŠ€', 'éŠ€è¡Œ'),
    '8306.T': ('ä¸‰è±ï¼µï¼¦ï¼ª', 'éŠ€è¡Œ'),
    '8308.T': ('ã‚Šããªï¼¨ï¼¤', 'éŠ€è¡Œ'),
    '8309.T': ('ä¸‰äº•ä½å‹ãƒˆãƒ©', 'éŠ€è¡Œ'),
    '8316.T': ('ä¸‰äº•ä½å‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8331.T': ('åƒè‘‰éŠ€', 'éŠ€è¡Œ'),
    '8354.T': ('ãµããŠã‹ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8411.T': ('ã¿ãšã»ï¼¦ï¼§', 'éŠ€è¡Œ'),
    '8591.T': ('ã‚ªãƒªãƒƒã‚¯ã‚¹', 'ãã®ä»–é‡‘è'),
    '8601.T': ('å¤§å’Œ', 'è¨¼åˆ¸'),
    '8604.T': ('é‡æ‘', 'è¨¼åˆ¸'),
    '8630.T': ('ï¼³ï¼¯ï¼­ï¼°ï¼¯', 'ä¿é™º'),
    '8697.T': ('æ—¥æœ¬å–å¼•æ‰€', 'ãã®ä»–é‡‘è'),
    '8725.T': ('ï¼­ï¼³ï¼†ï¼¡ï¼¤', 'ä¿é™º'),
    '8750.T': ('ç¬¬ä¸€ç”Ÿå‘½ï¼¨ï¼¤', 'ä¿é™º'),
    '8766.T': ('æ±äº¬æµ·ä¸Š', 'ä¿é™º'),
    '8795.T': ('ï¼´ï¼†ï¼¤', 'ä¿é™º'),
    '8801.T': ('ä¸‰äº•ä¸', 'ä¸å‹•ç”£'),
    '8802.T': ('è±åœ°æ‰€', 'ä¸å‹•ç”£'),
    '8804.T': ('æ±å»ºç‰©', 'ä¸å‹•ç”£'),
    '8830.T': ('ä½å‹ä¸', 'ä¸å‹•ç”£'),
    '9001.T': ('æ±æ­¦', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9005.T': ('æ±æ€¥', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9007.T': ('å°ç”°æ€¥', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9008.T': ('äº¬ç‹', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9009.T': ('äº¬æˆ', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9020.T': ('ï¼ªï¼²æ±æ—¥æœ¬', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9021.T': ('ï¼ªï¼²è¥¿æ—¥æœ¬', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9022.T': ('ï¼ªï¼²æ±æµ·', 'é‰„é“ãƒ»ãƒã‚¹'),
    '9064.T': ('ãƒ¤ãƒãƒˆï¼¨ï¼¤', 'é™¸é‹'),
    '9101.T': ('éƒµèˆ¹', 'æµ·é‹'),
    '9104.T': ('å•†èˆ¹ä¸‰äº•', 'æµ·é‹'),
    '9107.T': ('å·å´æ±½', 'æµ·é‹'),
    '9147.T': ('ï¼®ï¼¸ï¼¨ï¼¤', 'é™¸é‹'),
    '9201.T': ('ï¼ªï¼¡ï¼¬', 'ç©ºé‹'),
    '9202.T': ('ï¼¡ï¼®ï¼¡ï¼¨ï¼¤', 'ç©ºé‹'),
    '9432.T': ('ï¼®ï¼´ï¼´', 'é€šä¿¡'),
    '9433.T': ('ï¼«ï¼¤ï¼¤ï¼©', 'é€šä¿¡'),
    '9434.T': ('ï¼³ï¼¢', 'é€šä¿¡'),
    '9501.T': ('æ±é›»ï¼¨ï¼¤', 'é›»åŠ›'),
    '9502.T': ('ä¸­éƒ¨é›»', 'é›»åŠ›'),
    '9503.T': ('é–¢è¥¿é›»', 'é›»åŠ›'),
    '9531.T': ('æ±ã‚¬ã‚¹', 'ã‚¬ã‚¹'),
    '9532.T': ('å¤§ã‚¬ã‚¹', 'ã‚¬ã‚¹'),
    '9602.T': ('æ±å®', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9613.T': ('ï¼®ï¼´ï¼´ãƒ‡ãƒ¼ã‚¿', 'é€šä¿¡'),
    '9735.T': ('ã‚»ã‚³ãƒ ', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9766.T': ('ã‚³ãƒŠãƒŸï¼§', 'ã‚µãƒ¼ãƒ“ã‚¹'),
    '9843.T': ('ãƒ‹ãƒˆãƒªï¼¨ï¼¤', 'å°å£²æ¥­'),
    '9983.T': ('ãƒ•ã‚¡ã‚¹ãƒˆãƒª', 'å°å£²æ¥­'),
    '9984.T': ('ï¼³ï¼¢ï¼§', 'é€šä¿¡'),
}

# ================================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# ================================================================
@st.cache_data(ttl=3600)
def get_price(ticker, start, end):
    df = yf.download(ticker, start=start, end=end, progress=False)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.droplevel(1)
    return df

@st.cache_data(ttl=3600)
def get_benchmark(start, end):
    df = yf.download("^N225", start=start, end=end, progress=False)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.droplevel(1)
    return df

# ================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
# ================================================================
tab_analysis, tab_news, tab_market_news = st.tabs([
    "ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ",
    "ğŸ“° éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹",
    "ğŸŒ å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹",
])

# â”€â”€â”€ Tab1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_analysis:
    if st.button("â–¶ åˆ†æå®Ÿè¡Œ", type="primary"):
        end_date   = datetime.today()
        start_date = end_date - relativedelta(years=int(years))

        with st.spinner("å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥çµŒ225ï¼‰ã‚’å–å¾—ä¸­..."):
            benchmark = get_benchmark(start_date, end_date)

        if benchmark.empty:
            st.error("å¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            st.stop()

        market_returns = benchmark["Close"].pct_change().dropna()

        results = []
        progress    = st.progress(0)
        status_text = st.empty()

        for i, (ticker, (name, sector)) in enumerate(ticker_name_map.items()):
            status_text.text(f"å–å¾—ä¸­: {name} ({ticker})")
            df = get_price(ticker, start_date, end_date)
            progress.progress((i + 1) / len(ticker_name_map))
            if df.empty:
                continue
            returns = df["Close"].pct_change().dropna()
            common  = returns.index.intersection(market_returns.index)
            if len(common) < 30:
                continue
            x = returns.loc[common].values.flatten()
            y = market_returns.loc[common].values.flatten()
            annual_return = x.mean() * 252
            annual_vol    = x.std() * np.sqrt(252)
            beta   = np.cov(x, y)[0][1] / np.var(y)
            sharpe = (annual_return - risk_free_rate) / annual_vol
            results.append({
                "ä¼æ¥­å": name, "æ¥­ç¨®": sector,
                "å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)": annual_return * 100,
                "å¹´é–“ãƒªã‚¹ã‚¯(%)": annual_vol * 100,
                "ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª": sharpe, "ãƒ™ãƒ¼ã‚¿": beta,
            })

        progress.empty()
        status_text.empty()

        df_results = pd.DataFrame(results)
        if df_results.empty:
            st.error("ãƒ‡ãƒ¼ã‚¿ãªã—")
            st.stop()

        df_results = df_results.sort_values("ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª", ascending=False)

        st.subheader("ğŸ“‹ åˆ†æçµæœä¸€è¦§")
        st.dataframe(
            df_results.style.format({
                "å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)": "{:.2f}",
                "å¹´é–“ãƒªã‚¹ã‚¯(%)": "{:.2f}",
                "ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª": "{:.2f}",
                "ãƒ™ãƒ¼ã‚¿": "{:.2f}",
            }),
            use_container_width=True,
        )

        top_n_int   = int(top_n)
        top_stocks  = df_results.head(top_n_int)

        fig1, ax1 = plt.subplots(figsize=(14, 6))
        ax1.bar(top_stocks["ä¼æ¥­å"], top_stocks["ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª"], color="green")
        ax1.set_title(f"ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª ä¸Šä½{top_n_int}ç¤¾")
        ax1.set_ylabel("ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª")
        ax1.tick_params(axis="x", rotation=45)
        plt.tight_layout()
        st.pyplot(fig1)
        plt.close(fig1)

        fig2, ax2 = plt.subplots(figsize=(14, 6))
        ax2.bar(top_stocks["ä¼æ¥­å"], top_stocks["å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)"], color="steelblue")
        ax2.set_title(f"å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%) ä¸Šä½{top_n_int}ç¤¾")
        ax2.set_ylabel("å¹´é–“å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³(%)")
        ax2.tick_params(axis="x", rotation=45)
        plt.tight_layout()
        st.pyplot(fig2)
        plt.close(fig2)

        # AI ã‚³ãƒ¡ãƒ³ãƒˆ
        summary = top_stocks.head(5).to_string()
        prompt = f"""
ä»¥ä¸‹ã¯æ—¥æœ¬æ ªã®ãƒªã‚¹ã‚¯ãƒ»ãƒªã‚¿ãƒ¼ãƒ³åˆ†æçµæœã§ã™ã€‚
æŠ•è³‡å®¶å‘ã‘ã«ç°¡æ½”ã«300æ–‡å­—ä»¥å†…ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

{summary}
"""
        try:
            comment, ai_name = generate_ai_comment(prompt)
            st.subheader(f"ğŸ¤– AIã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ{ai_name}ï¼‰")
            st.write(comment)
        except Exception as e:
            st.warning(f"AI APIã‚¨ãƒ©ãƒ¼: {e}")

# â”€â”€â”€ Tab2: éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_news:
    st.subheader("ğŸ“° éŠ˜æŸ„åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»é©æ™‚é–‹ç¤º")

    # éŠ˜æŸ„é¸æŠ
    ticker_options = {f"{name}ï¼ˆ{t}ï¼‰": t for t, (name, _) in ticker_name_map.items()}
    selected_label = st.selectbox("éŠ˜æŸ„ã‚’é¸æŠ", list(ticker_options.keys()),
                                  index=list(ticker_options.keys()).index("ãƒˆãƒ¨ã‚¿ï¼ˆ7203.Tï¼‰") if "ãƒˆãƒ¨ã‚¿ï¼ˆ7203.Tï¼‰" in ticker_options else 0)
    selected_ticker = ticker_options[selected_label]
    selected_name   = ticker_name_map[selected_ticker][0]

    col_btn1, col_btn2 = st.columns([1, 4])
    with col_btn1:
        run_news = st.button("â–¶ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—", type="primary")
    with col_btn2:
        run_ai   = st.checkbox("ğŸ¤– AIã«ã‚ˆã‚‹è¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æã‚‚è¡Œã†", value=True)

    if run_news:
        with st.spinner(f"{selected_name}ï¼ˆ{selected_ticker}ï¼‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ä¸­..."):
            all_news = fetch_all_news(selected_ticker, selected_name, news_max_per_source)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã—ãŸã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰
        filtered = [n for n in all_news if n["source"] in show_news_sources] if show_news_sources else all_news

        source_colors = {
            "Yahoo!Finance JP":  "ğŸŸ¦",
            "æ ªæ¢(Kabutan)":     "ğŸŸ©",
            "ã¿ã‚“ã‹ã¶":          "ğŸŸ¨",
            "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰": "ğŸŸ¥",
            "æ—¥çµŒæ–°è":          "â¬›",
            "Reuters JP":        "ğŸŸ«",
        }

        if not filtered:
            st.warning("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            st.info(
                "**è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :**\n"
                f"- {selected_name}ï¼ˆ{selected_ticker}ï¼‰ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå„ã‚½ãƒ¼ã‚¹ã«å­˜åœ¨ã—ãªã„\n"
                "- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œè¡¨ç¤ºã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã€ã§çµã‚Šè¾¼ã¿ã™ãã¦ã„ã‚‹\n"
                "- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…ˆã®ã‚µã‚¤ãƒˆæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸ"
            )
        else:
            # éŠ˜æŸ„å›ºæœ‰ / å¸‚å ´å…¨ä½“ ã®å†…è¨³ã‚’è¡¨ç¤º
            ticker_specific = [n for n in filtered if n.get("ticker_specific", True)]
            market_wide     = [n for n in filtered if not n.get("ticker_specific", True)]

            col_a, col_b = st.columns(2)
            col_a.metric("ğŸ“Œ éŠ˜æŸ„å›ºæœ‰ãƒ‹ãƒ¥ãƒ¼ã‚¹", f"{len(ticker_specific)}ä»¶",
                         help="Yahoo!Finance/æ ªæ¢/ã¿ã‚“ã‹ã¶/TDnetã®éŠ˜æŸ„ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—")
            col_b.metric("ğŸŒ å¸‚å ´å…¨ä½“ï¼ˆéŠ˜æŸ„è¨€åŠã‚ã‚Šï¼‰", f"{len(market_wide)}ä»¶",
                         help="æ—¥çµŒãƒ»Reutersã‹ã‚‰éŠ˜æŸ„åãƒ»ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¨˜äº‹ã®ã¿æŠ½å‡º")

            # ã‚½ãƒ¼ã‚¹åˆ¥é›†è¨ˆ
            from collections import Counter
            src_counts = Counter(n["source"] for n in filtered)
            cols_stat  = st.columns(min(len(src_counts), 6))
            for i, (src, cnt) in enumerate(src_counts.items()):
                icon = source_colors.get(src, "âšª")
                cols_stat[i % len(cols_stat)].metric(f"{icon} {src}", f"{cnt}ä»¶")

            st.divider()

            # â”€â”€ éŠ˜æŸ„å›ºæœ‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å…ˆã«è¡¨ç¤º â”€â”€
            if ticker_specific:
                st.markdown(f"#### ğŸ“Œ {selected_name} éŠ˜æŸ„å›ºæœ‰ãƒ‹ãƒ¥ãƒ¼ã‚¹")
                for item in ticker_specific:
                    icon = source_colors.get(item["source"], "âšª")
                    badge = "ğŸŸ¥ **é©æ™‚é–‹ç¤º**" if item["source"] == "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰" else ""
                    title_short = item["title"][:70] + ("â€¦" if len(item["title"]) > 70 else "")
                    with st.expander(f"{icon} [{item['source']}]ã€€{title_short}"):
                        c1, c2 = st.columns([3, 1])
                        with c1:
                            badge_text = item.get("badge", "")
                            badge_map = {
                                "ç‰¹é›†": "ğŸŸ  ç‰¹é›†", "ææ–™": "ğŸŸ¢ ææ–™", "æ±ºç®—": "ğŸ”µ æ±ºç®—",
                                "é–‹ç¤º": "ğŸ”´ é–‹ç¤º", "ãƒ†ã‚¯": "âšª ãƒ†ã‚¯", "é€Ÿå ±": "ğŸŸ¡ é€Ÿå ±",
                            }
                            badge_label = badge_map.get(badge_text, f"â—¾ {badge_text}" if badge_text else "")
                            if badge_label:
                                st.caption(badge_label)
                            st.markdown(f"**{item['title']}**")
                            if item.get("summary") and item["summary"] not in ("ğŸ“„ é©æ™‚é–‹ç¤ºPDF", ""):
                                if not item["summary"].startswith("["):
                                    st.caption(item["summary"])
                        with c2:
                            if item.get("date"):
                                st.caption(f"ğŸ• {item['date']}")
                            if item.get("link"):
                                st.markdown(f"[ğŸ”— è¨˜äº‹ã‚’é–‹ã]({item['link']})")
                            elif item.get("source") == "TDnetï¼ˆé©æ™‚é–‹ç¤ºï¼‰":
                                st.caption("ï¼ˆPDFç›´ãƒªãƒ³ã‚¯å–å¾—ä¸­ï¼‰")

            # â”€â”€ å¸‚å ´å…¨ä½“ã‹ã‚‰éŠ˜æŸ„è¨€åŠã‚ã‚Š â”€â”€
            if market_wide:
                st.markdown(f"#### ğŸŒ å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆ{selected_name}ã«è¨€åŠï¼‰")
                for item in market_wide:
                    icon = source_colors.get(item["source"], "âšª")
                    title_short = item["title"][:70] + ("â€¦" if len(item["title"]) > 70 else "")
                    with st.expander(f"{icon} [{item['source']}]ã€€{title_short}"):
                        c1, c2 = st.columns([3, 1])
                        with c1:
                            st.markdown(f"**{item['title']}**")
                        with c2:
                            if item.get("date"):
                                st.caption(f"ğŸ• {item['date']}")
                            if item.get("link"):
                                st.markdown(f"[ğŸ”— è¨˜äº‹ã‚’é–‹ã]({item['link']})")

            if not ticker_specific and not market_wide:
                st.info(f"ç¾æ™‚ç‚¹ã§ {selected_name} ã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            # AI åˆ†æ
            if run_ai and filtered:
                st.divider()
                st.subheader("ğŸ¤– AI ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æï¼ˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼‰")
                with st.spinner("AIåˆ†æä¸­..."):
                    ai_result = ai_news_summary(filtered, selected_name, selected_ticker)
                st.info(ai_result)

# â”€â”€â”€ Tab3: å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_market_news:
    st.subheader("ğŸŒ å¸‚å ´å…¨ä½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥çµŒãƒ»Reutersï¼‰")

    if st.button("â–¶ å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—", type="primary"):
        import concurrent.futures

        with st.spinner("å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as ex:
                f_nikkei  = ex.submit(fetch_nikkei_market_rss, 10)
                f_reuters = ex.submit(fetch_reuters_jp_rss, 10)
                nikkei_news  = f_nikkei.result()
                reuters_news = f_reuters.result()

        col_n, col_r = st.columns(2)

        with col_n:
            st.markdown("### â¬› æ—¥çµŒæ–°è ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹")
            if nikkei_news:
                for item in nikkei_news:
                    st.markdown(f"- [{item['title']}]({item['link']})")
                    if item.get("date"):
                        st.caption(f"  ğŸ“… {item['date']}")
            else:
                st.info("å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆæ—¥çµŒæ–°èRSSã¯ä¼šå“¡åˆ¶ã®å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")

        with col_r:
            st.markdown("### ğŸŸ« Reuters Japan ãƒ“ã‚¸ãƒã‚¹ãƒ‹ãƒ¥ãƒ¼ã‚¹")
            if reuters_news:
                for item in reuters_news:
                    st.markdown(f"- [{item['title']}]({item['link']})")
                    if item.get("date"):
                        st.caption(f"  ğŸ“… {item['date']}")
            else:
                st.info("å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        # å…¨å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’AIã§è¦ç´„
        all_market = nikkei_news + reuters_news
        if all_market and st.checkbox("ğŸ¤– å¸‚å ´å…¨ä½“ã®AIè¦ç´„ã‚’è¡¨ç¤º", value=True):
            headlines = "\n".join(f"[{n['source']}] {n['title']}" for n in all_market[:12])
            prompt = f"""
ä»¥ä¸‹ã¯æœ¬æ—¥ã®æ—¥æœ¬æ ªãƒãƒ¼ã‚±ãƒƒãƒˆé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚

{headlines}

æŠ•è³‡å®¶å‘ã‘ã«ä»¥ä¸‹ã‚’æ—¥æœ¬èª300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
1. æœ¬æ—¥ã®å¸‚å ´å…¨ä½“ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼ˆå¼·æ°—/å¼±æ°—/ä¸­ç«‹ï¼‰
2. æ³¨ç›®ãƒ†ãƒ¼ãƒãƒ»ã‚»ã‚¯ã‚¿ãƒ¼
3. ä»Šå¾Œã®æ³¨æ„ç‚¹
"""
            with st.spinner("AIè¦ç´„ä¸­..."):
                try:
                    comment, ai_name = generate_ai_comment(prompt)
                    st.subheader(f"ğŸ¤– å¸‚å ´å…¨ä½“AIè¦ç´„ï¼ˆ{ai_name}ï¼‰")
                    st.info(comment)
                except Exception as e:
                    st.warning(f"AI APIã‚¨ãƒ©ãƒ¼: {e}")
